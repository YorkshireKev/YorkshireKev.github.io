[{"categories":["Games"],"content":"\nOne of the very first games I ever played on a home computer was a maze game called Labyrinth on the Sinclair ZX81. So when I wanted to learn Golang and delve a little deeper into terminal cli development, I decided to start with a simple maze game that could be run in a shell on the Linux command line.\nAs you can see from the screenshot, the graphics could, at best, be describes as very basic. But as a learning project, I’m quite pleased with the result.\nIf you’re interested in playing the game then you can download pre-built versions for Linux and MacOS from itch.io\nThe source code is available on my GitHub. Please keep in mind that this was a learning exercise, so don’t expect the code to meet professional standards ;-)\nIf you want to download the game directly onto a Linux host via curl or wget, then take a look at the repleases page on the above GitHub Repo for a direct link.\nAs I mentioned at the top of this post, CLI-Labyrinth was inspired by the classic 16k Sinclair ZX81 game called Labyrinth, originally written in 1981. If you enjoy maze games, it’s worth checking out the original - it’s much better than my “modern” version.\n","tags":["linux","cli","ssh"],"title":"CLI-Labyrinth","uri":"/cli-labyrinth/"},{"categories":["Linux"],"content":"While tidying the loft I came across my old JVC Mini DV Camcorder that I purchased in about 2001. I was looking to recycle an old, unused PC that was also in the loft when it occurred to me that the old PC had a firewire card installed. A quick check confirmed that the firewire card would not fit into my regular PC, so hanging on to this old PC for a while longer was my best chance of getting the footage off the old mini dv tapes.\nThe camcorder does have the ability to playback the tapes via composite output, but firewire is fully digital and thus would give me the best possible quality.\nAfter loading Ubuntu Linux onto the PC, I searched for an application that could pull the footage from the camcorder. I had no luck in finding anything with a nice GUI, so I was left with the good old command line!\nIt didn’t take long for me to stumble across a command line tool called dvgrab. This allowed me to control the camcorder via the Linux command line and, more importantly, to download the raw footage onto the PC.\nBy default the files are downloaded in raw dv format - excellent I thought - as this would preserve the footage exactly as it was recorded.\nSo after loading up a tape into the camcorder and connecting it to the PC, I fired up dvgrab in interactive mode using the following command:\ndvgrab -interactive If you press shift-? on the keyboard, dvgrab will show the keyboard commands that you can use in interactive mode. So I duly rewound the tape and pressed ‘c’ to start the capture. It was as simple as that. The tape played and files were written to the PC hard drive as a series of 1GB files.\nAfter the capture had completed I copied the files over to my main (Windows 10) PC and fired up HitFilm Express to do some editing. This is when things started to fall apart.\nAlthough I could play the files in media player and VLC, HitFilm didn’t recognise them at all. It did, however, suggest that I install quicktime. But after a bit of searching, it appears that quicktime for Windows has been deprecated for some years.\nI did find it odd that HitFilm was unable to import the files, especially as their website states that DV is supported.\nBack to Linux and dvgrab…\nAfter a spot of reading the man pages and a bit of experimenting, I found that dvgrab supports a slight variation on the dv format, called dv2. Apparently, this is the same raw data, but dv2 stores the audio and video separately, where the default, dv1, interleaves them.\nTo make dvgrab output in dv2 format simply requires a command line parameter passing to it on startup, namely - format dv2.\nCopying these newly exported files back to Windows and into HitFilme and bingo! it worked like a dream.\nWhile I was at it I added a few more options. These were: -timestamp, to have the filename include the timestamp of when the recording was made. -autosplit=600, to split make dvgram split the file whenever the recordings are more than 600 seconds apart. This is in addition to splitting every 1gb.\nThis was the command line I ended up using with success: dvgrab -interactive -format dv2 -autosplit=600 -timestamp filename- The last part, filename- is just what dvgrab will write as the prefix for the filename, so you can omit it or change it to something appropriate to your recording e.g. summer-holidays-\n","tags":["fxhome","hitfilm","dvgrab"],"title":"Exporting footage with dvgrab to edit with fxhome HitFilm Express","uri":"/dvgrab-with-hitfilm-express/"},{"categories":["Games"],"content":"\nOne of my all-time favourite games is Manic Miner on the ZX Spectrum. When Bug-Byte released Matthew Smith’s masterpiece in 1983 the bar was raised for quality gaming on the Speccy. Sure there were many, many excellent games released on the Spectrum (I’m looking at you Ultimate) but Manic Miner is still my go-to game when I need a retro gaming fix.\nEver since I saw Mario64 on the N64 I wondered what a 3D version of Manic Miner or Jet Set Willy would look and play like. After all, if Mario could make the jump to 3D, why not Willy?\nSo with Covid19 removing most of my excuses for not doing a spot of coding in the evenings I thought I’d try making a 3D version of Manic Miner. Well, not the whole game, just the first level to try and get a flavour of what a 3D manic miner might look like.\nHaving cut my teeth with the Godot game engine on my 2D shooter Retrocadia, I’m feeling confident I can get something working… I mean, writing a 3D game… how hard could it be right?\nThis page is my mini-blog/development diary. I’ll add sections at the bottom of the page as I make progress with the game.\nUpdate - I’ve now completed making the game.\nCentral Cavern has been released for Windows, Mac and Linux.\nDownload it from itch.io: yorkshirekev.itch.io/central-cavern\nDownload it from gamejolt: gamejolt.com/games/central-cavern/532409\nHere are some screenshots of the finished version:\n…and a short video of the game:\nFAQ Is this going to be a full 3D version of Manic Miner? No, I’m only going to do the first level, Central Cavern. Hence the name of this project is called Central Cavern. I don’t think I’m up for the slog of doing more than that :/\nWhen do you expect the game to be released for download? The game is available for download from itch.io: yorkshirekev.itch.io/central-cavern or from gamejolt: gamejolt.com/games/central-cavern/532409\nWhat software are you using to make the game? Well, the main game logic etc will be written using the Godot game engine, with GD Script for the programming language. For the graphics, I’m using Blender. For sound, I’ll probably use audacity to edit sound files etc, and for any 2D graphics, Gimp is my go-to choice of editor.\n6-Jun-2020 - The First Update! Off to a flying start! Okay, so I’ve been working on for a few weeks and not posted anything yet!. Over the last few weeks, I’ve managed to make the basic level structure in Blender and modelled the characters for the Guardian and Miner Willy. Actually, I’d done the Guardian a year or two ago do I just dug it out, imported it into the latest version of blender and added the rigging for animation. I’ll probably redo Miner Willy at some point as I’m not happy with how he’s turned out.\nAnyway, I’ve now got a very basic working program. Obviously, there is loads more work to do before it becomes anything like a working game, but I’m quite pleased with the progress so far and I’ve really enjoyed making it.\nHere’s a short video of my progress so far…\n25-Jun-2020 - Collapsing Platforms, Keys and More Over the last 3 weeks, I’ve made a fair amount of progress on and off. I’ve got the basic game working, with collapsing platforms, keys. Willy can now be killed by hitting the guardian or a spikey plant etc, or even by falling too far.\nI’ve had to adjust some of the platforms to make the game possible to complete.\nOne of the trickiest things I’ve been trying to sort is the camera. I’ve added a second camera view that gets toggled to when the main camera view gets blocked. You can select between them manually too. This is much better than before, but still not quite an good as I want it to be.\nI’ve still got a fair bit to add yet. There’s more sound effects and, of course, some music. Not to mention a title screen and the game over boot scene. Hopefully, I’ll have another update in a couple of weeks. Thanks for looking!\nHere is an updated video showing the progress so far…\nUpdate 4-Jul-2020 - Music and Sound and a Title Screen! I’ve now added a simple title screen to the game with music taken from a 1936 recording of Johann Strauss’s Blue Danub played by Finnish orchestra Rytmi-Pojat, directed by Eugen Malmstén. I’ve also managed to find a free version of Hall of the mountain King recorded by Kevin MacLeod, which I’ve trimmed to use as the in game tune.\nI had to resort to doing a spot of coding on the ZX spectrum to get an authentic jumping sound. It must have been 30+ years since I’ve written anything in zx basic!\nThere have been a few other tweaks here and there, and I’m starting to see light at the end of the tunnel. I still need to add a game over screen, something for when the player completes the game and I’ll probably faff with the lighting to try and give it a more underground feel if I can.\nUpdate 12-Jul-2020 - Just about finished it After a final flurry of activity, I’ve just about completed writing the game. There’s probably a little bit of polish to add, but it’s now a complete and playable game! Okay, so it’s only got one level but as I always said, that’s all I was ever going to write. I’ve added a game over screen, and a nice surprise if the player ever manages to complete the level.\nMy next task is to add some screenshots and make a short video of the gameplay. I then plan to seek permission from the copyright owners of Manic Miner (if I can find them) to see if I can release the game as a free download. Wish me luck!\nUpdate 21-Aug-2020 - Out in the wild! I have finally decided to make central-cavern available for download. I’ve uploaded the game to itch.io and I will also upload to gamejolt once their site comes back online.\n","tags":["godot"],"title":"Godot Experiment no. 2, Central Cavern","uri":"/central-cavern/"},{"categories":["Programming"],"content":"These are a bunch of VS Code editing keyboard combinations that I find useful, but often struggle to recall. I’ll add more as I discover (and need) more editing features…\nMulti line select and edit. Highlight some text then use the keyboard combination below to select for simultaneous editing.\nMacOS: Shift + Cmd + L\nWindows / Linux: Shift + Ctrl + L\nSelect cursor on mutliple lines Select a block of multiple lines. Then use the key combination below to put cursors at the end of every line. MacOS: Shift + Alt + I\n","tags":["vscode","ide"],"title":"Useful VS Code Keyboard Combinations","uri":"/useful-vscode-keyboard-combinations/"},{"categories":["Games"],"content":"\nMost of my recent (and not so recent) programming efforts have been in JavaScript that runs in the browser. I believed that with WebGL, the browser would become more relevant in gaming and graphics applications. Fast forward a few years and it’s clear that the vast majority of PC games are still downloaded and installed, just as they were decades ago.\nI decided to return to creating native games for the PC. It’s been almost 20 years since I last wrote a native PC game, so I thought I’d start with something simple to get my eye in.\nThe first thing I needed to do was figure out what tools I’d use to create the game. Back in the 90s, I used to code in C and C++, using Open GL and Direct X for the graphics and sound. But these days a PC doesn’t only mean Windows. Linux and Mac need to be supported too.\nToday the vast majority of games are created using a game engine. A game engine takes care of most of the boilerplate code allowing game creators to concentrate on the game’s logic rather than worrying about graphics rendering, sound playback routines, and other framework features.\nGiven my fond memory of C++, I decided to give Unreal Engine a closer look. Much to my surprise, I found coding game logic in C++ to be quite time consuming and verbose. It turns out that going back to a fairly low-level language was not as much fun as I thought it was going to be. Another issue I have with unreal is that it’s not open source. I don’t currently plan to sell the games I write, so the Unreal license would be free of charge for me, but I can’t quite get comfortable with using a commercial license. As fantastic as Unreal Engine is, I decided it wasn’t the engine for me.\nNext up I gave Godot a look. Godot is a full-featured 2D/3D game engine that is open source. The game logic is written in Godot’s GD Script, though there is a growing number of languages supported by the GD Native extension. GDscript is quite Pythonesque, so it has a reasonably shallow learning curve. After running through the official tutorial: Dodge the Creeps! I decided to go with Godot for my return to native PC game programming.\nIntroducing Godot Experiment no. 1, Retorcadia Retrocadia has been released for Windows, Mac and Linux.\nDownload it from itch.io: yorkshirekev.itch.io/retrocadia\nRetrocadia is a simple 2D shoot ’em up. It is very loosely inspired by the ZX Spectrum classic: Arcadia.\n(Arcadia - ZX Spectrum Screenshot)\nI’ve had a few challenges using the Godot engine. But these were nothing too major and typical of learning something new. An example of this was getting the enemy’s path to loop seamlessly. The followpath2d provided by the engine is intended to follow the path from one end to the other. When looped it just jumps back to the starting screen position. I got past this by monitoring for when the sprite was at the end of the path, then adjusting the start position of the path as it looped.\n(Retrocadia PC Screenshot)\nThe game is now almost complete. I’ve still got to add a few finishing touches such as explosions when the player is hit. It’s also only got 4 levels, so I need to add a few more I think!\nUpdate 02/10/2019 I’ve added a naff explosion effect when the player gets hit and made the enemy bullets get faster after every level completed (well, up to a limit!). The game is about as complete as I’m going to make it, apart from adding a few more levels. I’ll then need to figure out the best way to release the game. I’ll probably put it on itch.io for download.\nUpdate 06/10/2019 Added one more level and tidied up a few loose ends. I’ve also added a check that will inform users if there is a newer version of the game available to download. This allows me to release it as beta and let users know when I update it e.g. when I get round to adding more levels as well as fixing any bugs that might come to light. Retrocadia v0.1 released for Mac and Linux. A Windows version will follow when I find a windows PC to build and test it on! You can download it from itch.io: https://yorkshirekev.itch.io/retrocadia\nUpdate 10/10/2019 The Windows version is now also available for download from itch.io alongside the Mac and Linux versions.\nI have also built a web version so the game can be played in a modern desktop browser. At the time of writing the game worked in the latest versions of Chrome and Firefox, but sadly didn’t load in Safari. I’d say the web version should be considered experimental and loading can be a little hit and miss. In chrome on my Mac, for example, loading seemed to hang but then ran just fine when I hit refresh! I guess this sort of thing is to be expected as Web-Assembly and Godot’s ability to generate it are both cutting edge experimental features. Click here to play Retrocadia in the browser\n","tags":["godot","javascript"],"title":"Godot Experiment no. 1, Retrocadia","uri":"/retrocadia/"},{"categories":["Retro","Commentry"],"content":"Matthew Smith, the author of the classic ZX Spectrum games Manic Miner and Jet Set Willy was at the Play Expo 2019 retro gaming event in Manchester to do a talk marking the 35th anniversary of Jet Set Willy. I was lucky enough to attend this event and had taken some of my old cassette covers with me, just in case Matthew was up for doing some signing after his talk. So at the end of the talk I legged it up to the stage, joining the unofficial queue of fans seeking autographs and selfies. Did I ask him to sign my covers of Styx, Manic Miner and JSW? Too right I did!\nWhen I presented my cover of Styx for the obligatory autograph, Matthew seemed pleasantly surprised to see it. I guess most people only rock up with Manic Miner or JSW cases. He mentioned that it was the money from Styx that allowed him to write Manic Miner, and then the money from Manic Miner allowed him to write Jet Set Willy. I think he said he got £8,000 for Styx (there was a lot of noise, so I could be mistaken about the amount).\nThe next day (it was a 2 day event) I went to take a look at the Spectrum Next stand, and there was Matthew Smith checking out the yet to be released Next. Jim Bagley loaded up Manic Miner and Matt had a quick game. He didn’t get very far though. In fact he didn’t even collect one key! Mind you, he was getting pestered by fans at the same time.\nAt this point I asked him if I could take the obligatory selfie with him, which I was delighted to say he agreed to. We then got chatting about whether he fancied doing something for the next or spectrum. He said he had planned to make about 20 spectrum games back in the 80s, something like 2 to 3 a year, but after not getting paid for JSW his computers quickly fell into disrepair. He said he was working on something, or maybe he was just thinking of working on something; I couldn’t quite make out what he meant. Matthew did said he wouldn’t make another Miner Willy game. When I asked why he said that there is too much hassle over copyright. “I own the copyright, but I have to fight for it. So it’s just less hassle if I do something else”. It would be great to see another Matthew Smith game, and I’m sure he could raise all the funding needed with a kickstarter, but I don’t think I’ll be holding my breath…\nI also asked him about Delta Tao One, which is the first game Matthew had published. It was on the TRS-80. He told me how he always developed on the TRS-80 because it had a decent keyboard and disk drives. He wrote both Manic Miner and JSW on the trs-80, and built an interface board to squirt the compiled code onto the spectrum. This allowed him to re-load the code onto the spectrum in seconds after a crash or code change. I told him that getting hold of a copy of Delta Tao One was nigh on impossible. He mentioned some website or other and was going to look on his phone, but the battery was dead. He did say if I managed to find a copy I have his blessing to take a copy of it.\nI was 13 or 14 when Manic Miner and Jet Set Willy were first released. I was instantly hooked and I can honestly say those were the games, more than any others, that inspired me to learn about computers and programming. 35 years on and it was an absolute pleasure to finally meet and chat with one of my childhood heroes, Matthew Smith. An absolute legend.\n","tags":["jet-set-willy","manic-miner","spectrum","styx","matthew-smith"],"title":"Chatting with Matthew Smith","uri":"/chatting-with-matthew-smith/"},{"categories":["Linux"],"content":"I recently started using cassette tape emulators to load games on my retro zx Spectrum and C64 (search casduino and tapduino if you’re interested in these). Basically these work by holding the tape images as files on an SD or Micro SD card and the casduino then plays them as audio into the old computers as though a tape deck was connected. They work great but one issue I found was that the files did not list in the expected order when viewing through the casduino menu.\nIt turns out that the FAT filesystem (FAT12, FAT16 and FAT32) list directories in the order that files were written to the filesystem, and it is the OS that re-orders them.\nFor systems that don’t sort the files before displaying, like casduino and some car audio players, it can be difficult to find the file you’re after when they are not in alphabetical order.\nAfter a fair amount of searching I discovered an open source program called fatsort written by Boris Leidner. Fatsort is a command line utility that modifies the order of directories and files on a FAT formatted SD card.\nThe instructions on fatsort’s website https://fatsort.sourceforge.io/ has instructions for compiling and installing from sourcecode, but for Linux users there is a pre-built version in most distribution repositories and Mac users can install using brew.\nUsing the command is fairly simple with the trickiest part being identifying the sd card device so we can point fatsort at it.\nStep 1 – Locate the SD card. On both Linux and Mac you can use the mount command to find the device of the SD card. Insert the SD card and let it mount as usual. Then type mount to see where the card was mounted.\nmount /dev/disk1s1 on / (apfs, local, journaled) devfs on /dev (devfs, local, nobrowse) /dev/disk1s4 on /private/var/vm (apfs, local, noexec, journaled, noatime, nobrowse) map -hosts on /net (autofs, nosuid, automounted, nobrowse) map auto_home on /home (autofs, automounted, nobrowse) /dev/disk1s2 on /Volumes/Preboot 1 (apfs, local, journaled, nobrowse) //Time%20Machine%20User@synology-nas._afpovertcp._tcp.local./Time%20Machine%20Folder on /Volumes/Time Machine Folder (afpfs, nobrowse) /dev/disk3s1 on /Volumes/ZX_SPECTRUM (msdos, local, nodev, nosuid, noowners) $ Your output will differ from mine, but you should be able to spot your sd card. In the example above (on my Mac) the ZX_Spectrum SD card was mounted as /Volumes/ZX_SPECTRUM. The bit we’re interested in the the device path of the mounted sd card. In the example above this is /dev/disk3s1. Take a note of the device path (e.g. /dev/disk3s1 in the above example) or copy it to the clipboard as we’ll need it later. Linux output will be very similar.\nStep 2 – Unmount the SD card Next we need to unmount the sd card before we can run fatsort on the device. You can do this by using the umount command or simply use the remove/eject options from the desktop.\nsudo umount /Volumes/ZX_SPECTRUM Step 3 – Run fatsort Now we are ready to run fatsort.\nfatsort {device path} ..using the above example device would look like this. fatsort /dev/disk3s1 And that’s it! After a few seconds the filesystem should be converted into alphanumeric order. There are various options to refine how the sort will operate (use fatsort -h or man fatsort for details). However, the default options were just fine for me.\n","description":"How to fix the file listing order on fat32 sd cards","tags":["sd-card"],"title":"Fix file ordering on fat32 sd cards","uri":"/fix-file-ordering-on-fat32-sd-cards/"},{"categories":["Programming","Electronics"],"content":"The ESP8266 isn’t an official Arduino, but the community has created a package so that it can be programmed from the Arduino IDE, and allow it to use the Arduino libraries.\nThis blog post describes how I set up my development environment on my Macbook Pro running High Sierra.\nThe first thing to do is to download and install the IDE. At the time of writing the latest version was 1.8.6\nOnce the IDE is installed, open the editor and then open preferences (under the Arduino menu) and paste this URL into the ‘Additional Boards Manager URLs’ field.\nhttp://arduino.esp8266.com/stable/package_esp8266com_index.json\nNext, we need to install the package so that we can use it. Go to the tools menu, select board manager and install the ESP8266 community package. The latest version should do!\nIn theory that’s all there is to setting up a development environment to support the ESP8266. However, when I tried to push a program to the board the USB-Serial port did not recognise the connected device.\nAfter a bit of internet searching I found some posts suggesting that the serial chips used on most of these cheap Chinese made development boards require drivers. Mine contained a CP2102 but other common boards can use a CH340G\nI found a great page that shows the chips used on various boards. I’ve copied the main image below for quick reference:\nFor me I needed the CP210x drivers for ESP8266 serial com usb to uart bridge. They can be downloaded from this silabs website page.\nHowever, this still didn’t work for me! It turns out that my mac was blocking the driver being loaded into the kernel. There was no warning or notification about the driver being blocked, so it took me a while to figure this out! To fix this you need to go to setting, then click on the security \u0026 privacy icon. At the bottom of the panel it says the driver is blocked. Click on the allow button and hey presto, it now works!\nAfter allowing the driver to be loaded into the kernel, everything worked just fine for me.\nStill having problems? Here are a few extra things you could try.\nMake sure you use a decent USB cable! This caught me out and took half an evening for me to figure out. I returned to a project and could not get the IDE to detect my ESP board. I re-installed the drivers and checked the above security settings but still it would not detect the USB serial. I eventually switched USB cables and guess what, it worked!\nThe other common serial chip used on cheap Arduino clones is the CH340. You can download drivers from this git repository. You can also get them from the originating Chinese website. Remember, you may also need to allow this driver to load in the security \u0026 privacy dialog.\nI had also read that the FTDI drivers might need to be updated. These can be downloaded from http://www.ftdichip.com/Drivers/VCP.htm\n","description":"ESP8266 MacOS Arduino IDE Setup","tags":["arduino","esp8266"],"title":"ESP8266 MacOS Arduino IDE Setup","uri":"/esp8266-macos-arduino-ide-setup/"},{"categories":["Programming","Electronics"],"content":"I recently built a remote temperature sender based around an Arduino Nano. The coding is simple enough; read the temperature from the attached sensor, transmit the value over a 433mhz transmitter pause for a bit and repeat.\nMy first cut of code used the standard delay() command to pause before looping back for another reading. delay(8000); //Sleep(ish) until next temprature reading is required. Hooking up my multimeter I recorded the current draw at about 37-38 milliamps from 3xAA batteries - 4.5 volts. The current draw increased slightly to around 45 milliamps when the Arduino was transmitting the data.\nThe Arduino’s microprocessor, an Atmel Atmega chip, can be put into a sleep mode where its internal watchdog timer will trigger after a defined period of time and wake the chip up.\nProgramming the watchdog timer and switching off all of the peripheral components of the Atmega chip can be quite complicated. Fortunately there are a few libraries that make the task super simple. The one I chose to use is called low-power by rocketscream. https://github.com/rocketscream/Low-Power\nUsing this library the code changes are minimal. Just include the library and replace delay command with a single low-power command. include \u003cLowPower.h\u003e LowPower.idle(SLEEP_8S, ADC_OFF, TIMER2_OFF, TIMER1_OFF, TIMER0_OFF, SPI_OFF, USART0_OFF, TWI_OFF);\nWith this simple change, the current draw dropped significantly - to around 21 milliamps except for when the data was actually being transmitted.\nWhile the power savings are impressive, there are a few downsides to putting the Arduino to sleep during pauses. If your application relies on background interrupts, such as listening for data on an rf receive module, then your code will most likely miss the transmission. So in my case, using sleep between transmissions is a good use-case for sleep, but the temperature receiver is not.\nOne other minor annoyance is that the Atmega can only sleep for a maximum of 8 seconds when using the watchdog timer. If you need to pause for longer than that then you’ll have to use a loop.\n//Loop for 6 times (48 seconds between temprature transissions) for (int i = 0; i \u003c 6; i++) { LowPower.idle(SLEEP_8S, ADC_OFF, TIMER2_OFF, TIMER1_OFF, TIMER0_OFF, SPI_OFF, USART0_OFF, TWI_OFF); } A note for Atmega168 users At the time of writing the low-power library does not compile with the older Atmega168 based Arduinos. A fix should hopefully get pulled into the next release but in the meantime I have made a patched fork of the code available on my GitHub . Although the 168 is now considered obsolete I do find the ridiculously cheap Chinese clones to be far to tempting to ignore!\n","description":"Arduino Power consumption measurements between Delay and Sleep","tags":["arduino"],"title":"Arduino power consumption - Delay vs Sleep","uri":"/arduino-power-consumption-delay-vs-sleep/"},{"categories":["Docker","Programming","Electronics"],"content":"Update - Feb 2020 These instructions are no longer required for macOS Mojave 10.14 or later. Updating to the latest macOS version should be enough to make Nanos based on CH340G chipset work. In fact, installing these drivers on macOS Mojave 10.14 or later could cause OS issues!\nOriginal Post I recently bought some really cheap Arduino Nanos. These were based on the CH340G chipset. Unfortunately, when I loaded the Arduino editor and tried to push code onto one of them, I discovered that there was no USB serial driver available to select.\nThe Mac already has Arduino USB drivers pre-installed and I’ve not had problems uploading code before. It turns out that some of the really cheap nano clones use different serial loaders to those used by genuine arduinos (or more expensive clones). My guess is that this is done to reduce licensing costs.\nThe Nano clones I bought cost me less than £3 each so I can’t really complain if I need to mess about a bit to get them working.\nThere have also been reports that plugging in a nano clone that is based on any of the CH340 chipsets causes the kernel to panic in macOS Sierra. This appears to be the case if you have installed compatible drivers on a previous version of macOS.\nIt turned out to be really easy to get these cheap nanos working by installing a compatible USB serial driver. The hardest part was finding a working driver that is compatible with macOS Sierra. The driver I ended up using can be downloaded freely from this GitHub repository\nWarning: Do not install if you have the current macOS Mojave 10.14 or later.\nThe instructions in the above repo explain how to install the drivers. In my case, I didn’t have any kernel panic issue to resolve so I didn’t need to manually delete any drivers. I simply ran the pkg file and rebooted my MacBook.\nAfter installing the new driver I was able to upload code to both the CH340G based nano clone and the standard nano.\n","tags":["arduino"],"title":"Using cheap arduino clones with macOS Sierra","uri":"/macos-sierra-driver-for-arduino-clones-ch340g/"},{"categories":["Docker","Programming"],"content":"These are a bunch of docker commands that I find useful for keeping things tidy.\nRemove all orphaned images. These are the images tagged as \u003cnone\u003e. docker images -q --filter \"dangling=true\" | xargs docker rmi\nDelete/remove all stopped containers. Note that -v option; it will also delete any persistent volumes that belong to the stopped containers. docker ps -aq --filter \"status=exited\" | xargs docker rm -v \u0026\u0026 docker ps -aq --filter \"status=created\"\n","tags":["docker"],"title":"Useful Docker Commands","uri":"/useful-docker-commands/"},{"categories":["Mac"],"content":"I have recently switched to a MacBook Pro as my daily driver. One of the first things I noticed was that there was no maximise button on application windows, only a fullscreen option that hides the top status-bar. A quick google search told me that double clicking the title-bar or holing down ALT while clicking in the Green Icon would maximise the window instead of making it go fullscreen. Which is great! only it isn’t :(\nWhat that actually does is make the window wide enough for it’s content, but that does not usually mean full screen width.\nI eventually discovered that holding down SHIFT + ALT while clicking on the Green Icon does make the window fill the screen width and height.\nALT + SHIFT + Green Icon = Maximise (as you’d expect if on Windows/Linux).\nNow I’ve got that sorted, I’m starting to wonder if the fullscreen option is a better way of working anyway…\n","title":"Maximizing a window on a MacBook","uri":"/mac-os-maximize-window/"},{"categories":["Programming"],"content":"For my latest javascript/threejs experiment I created a retro 1980’s inspired demo, which renders hundreds of sprites in a pseudo 3D style that was common in the Atari ST and Amiga demos of the late 80’s.\nOf course, no retro demo would be complete without a retro soundtrack. For my Retro Particle Demo this would be a short track that loops continuously.\nMy initial attempt used the html5 audio tag: \u003caudio id=\"demomusic\" src=\"audio/music.mp3\" type=\"audio/mpeg\" loop\u003e\u003c/audio\u003e\nThe above tag will load the mp3 file, but not start it playing (you’d need to add autoplay attribute for that). I didn’t want the track to start playing until the demo was loaded, so no autoplay for me.\nThe loop tag tells the browser to keep playing the track over and over again, which didn’t work out so well as I will explain below.\nStarting the track from my JavaScript code was simple enough. Just call the play method: document.getElementById(\"demomusic\").play();\nLooped music can get a bit annoying after a while so I added the option to mute the sound. The following onclick event toggles the music on/off by calling the pause and play methods of html5 audio. document.getElementById(\"music\").onclick = function () { if (mute === true) { document.getElementById('demomusic').play(); mute = false; } else { document.getElementById('demomusic').pause(); mute = true; } };\nWhile the html5 audio tag works quite well it does have one annoying aspect: there is a slight delay/pause in-between loops.\nWhile looking for a fix to this I learnt that part of the ‘pause’ problem is caused by using mp3 as the audio file format. Apparently there is a bug (or maybe its a feature!) in the mp3 spec that adds a few ms of silence at the start of the track. If you load a track into an editor like audacity you can see the gap. You can edit the track to remove the silence, but it’ll be back when you re-load it. I recommend using the ogg format instead of mp3 as ogg does not add any silence to the start of the track. Switching to the ogg format certainly made an improvement but there was still a small pause between loops.\nIncidentally just converting from mp3 to ogg won’t remove the silence automatically. You’ll need to load the mp3 into an editor, remove the gap from the start of the file, then export the track in the ogg format.\nSo it looks like for now the html5 audio tag is not suitable for seamless looping.\n###Goodbye html5 audo tag, hello web audio!###\nThere is a newer ‘web audio’ api supported by most modern browsers. This looks to have a much richer feature set than html5 audio and does not suffer from pauses in looped playback (as long as you don’t use mp3 for the reasons described above). Web audio is more complicated to implement though.\nThankfully there are a number of JavaScript frameworks to simplify things. My personal favourite is the excellent howler.js\nHowler.js defaults to the Web Audio API but will fall back to html5 audio for browsers that don’t support the web audio api.\nTo use howler.js, add the JavaScript library in your html and don’t forget to remove the html5 audio tag. The audio files will be referenced in the javascript file instead: var music = new Howl({ urls: ['audio/music.ogg'], autoplay: false, loop: true }); var mute = false;\nThe rest of the JavaScript is pretty similar to that used for html5 audio.\nTo start the track playing: music.play();\nand the pause/play function now looks like this: document.getElementById(\"music\").onclick = function () { if (mute === true) { music.play(); mute = false; } else { music.pause(); mute = true; } };\nThere is so much more that howler.js can do than seamlessly looping music. Check out the howler.js website for details.\n","tags":["javascript"],"title":"Seamless audio looping in html5 JavaScript","uri":"/seamless-audio-looping/"},{"categories":["Games","Demos"],"content":"External page loaded from seperate github pages repo.\n","description":"A retro demo inspired inspired by the Atari ST demo scene of the late 1980's","title":"Retro Particle Demo","uri":"/retro-particle-demo/"},{"categories":["Games","Demos"],"content":"External page loaded from seperate github pages repo.\n","description":"A simple browser based slot machine game written using the excellent three.js javascript library.","title":"Virtual Slot Machine","uri":"/virtual-slot-machine/"},{"categories":["Games"],"content":"I’ve been experimenting with the html canvas element recently by writing some simple 2D games. One challenge is making a canvas based game fit nicely on the screen given the vast variety in browser resolutions. 2D games tend to be made up of images and I found that scaling these images so that they fill the browser window was both slow and ugly.\nThis got me thinking about WebGL. Is it possible to use a 3D WebGL library to render a 2D game and gain the scaling and performance advantages of the GPU? At first I looked at three.js, which is probably the most popular WebGL library. But in doing my research I discovered a dedicated 2D JavaScript WebGL library called Pixi.js.\nPixi.js provides a JavaScript API for 2D rendering, sprites and scaling using WebGL and will automatically fall back to the canvas rendering if WebGL is not available. This sounded ideal so I figured I’d give it a closer look.\nAfter studying some of the examples on the Pixi.js website I created a few dodgy looking demos (these are safely hidden away in a dark corner of my hard drive to save me some embarrassment). Now that I had a basic understanding of how Pixi.js worked I thought I’d have a go at writing a simple 2D game.\nYou can take a look at the game I created by visiting the game page. The source code is available on my github page.\nI found the Pixi.js api to be extremely well thought out and predictable to use. Performance seems good, even on my ageing AMDx2 Desktop PC. Scaling the graphics to match the browser window was a little trickier that I expected as I soon realised that I’d have to make adjustments to keep the correct aspect ratio of my badly drawn graphics. Just scaling to the browser window size tends to stretch the image.\nConsidering that this was my first serious attempt with Pixi.js and that I’m still getting to grips with JavaScript, all in all I’m quite pleased with the result.\n","description":"A simple 2D puzle game written in JavaScript with the Pixi.js framework that runs in the browser.","tags":["javascript"],"title":"A look at Pixi.js","uri":"/a-look-at-pixi-js/"},{"categories":["Programming"],"content":"This is a simple JavaScript keycode lookup page. Just press a key and the JavaScript Key Code will be displayed in the box below.\nIf you want to stop your browser scrolling when you press space or arrows etc then click in the input field to take the focus away from the whole page.\nThe text equivalent of the key being pressed is only displayed in browsers that support event.key. Rather surprisingly this does not include Chrome!\n","description":"Online JavaScript keycode lookup utility","tags":["javascript"],"title":"JavaScript KeyCodes","uri":"/javascript-keycodes/"},{"categories":["Programming","Retro"],"content":"A list of machine emulators completely written in JavaScript that I find to be both impressive and interesting. Writing an emulator in any language is difficult and requires some serious programming chops. But to do it in JavaScript must surely require a level of programming genius (or maybe madness - I hear it’s a fine line!) that us mere mortals can only dream of aspiring to.\nThese are my favourites that I’ve stumbled across, catalogued here so I can easily find them again whenever I feel the need for a retro computing fix…\nThese all need a modern (aka html5) browser with a fairly modern PC.\nSinclair JSSpeccy - ZX Spectrum qaop/js - ZX Spectrum fullscreen. Click on sides of window for menu JtyOne - ZX81 Emulator, with games! 3D Monster Maze anyone? Atari EstyJS - Atari ST Apple PCE.js - Apple Mac System 7 A2 - Apple 2 with games Acorn jsBeeb - BBC Model B / Master Commodore Scripted Amiga Emulator - Amiga emulator with some demos and free games PC Linux Virtual X86 - Linux and old versions of Windows with GUI jsLinux - Command Line Linux Ms-Dos js-dos - MS Dos in JavaScript. Includes a working copy of DOOM! Nintendo jsnes - Online NES emulator, includes some classic games Misc The author of the qaop/js spectrum emulator also has a page full of emulator links that is well worth checking out! Frederic Cambus’s github page also has a nice selection of emulator links. ","tags":["javascript"],"title":"Emulators written in JavaScript","uri":"/emulators-written-in-javascript/"},{"categories":["Programming","Games"],"content":"Having not written much client side JavaScript for at least a couple of years I thought I’d re-introduce myself to the language, and to the html5 canvas element.\nI decided to spend a few hours building a html5 version of the old ZX Spectrum Classic, Lightcycles (1983), which itself was based on Tron. This isn’t exactly a faithful reproduction, it’s more inspired by my memory of the game than it is a conversion of it.\nThe object of the game is simple enough. You can either play against the computer or against another player. 2 play mode is done old school style and requires both players to share the keyboard - no online network gameplay here!\nThe aim of the game is to race and trap your opponent. Keep blocking them until they crash into either the edge of the grid or a laser trail.\nIf you fancy a quick game, click here to play html5 LightCycles. The sourcecode is available on GitHub.\nA few things I learnt while writing this simple game:\nNamed global objects are a good way to minimising global variable leakage. Property names of objects don’t minify/uglify at all, only the object name gets mangled. Drawing to the canvas is slower and takes more CPU resources than I had expected. To get round the CPU load I kept two in memory arrays for the game grid and only drew the differences between the current and last frame to the canvas. I also checked against the grid array for collisions, as checking the canvas directly is even more expensive than drawing to it. There is probably a better way than to draw boxes for each pixel, but I wanted the display to scale to match the screen resolution.\nAll in all, I had en enjoyable afternoon writing the game. It’s no masterpiece for sure, but it was a useful JavaScript refresher!\n","description":"The classic zx spectrum game lightcycles recreated in JavaScript that runs in the browser.","tags":["javascript"],"title":"More adventures in JavaScript","uri":"/more-adventures-in-javascript/"},{"categories":["Cloud","Docker","Linux"],"content":"Sometimes, when setting up and debugging a container it is extremely useful to be able to ‘shell into’ the container to get a closer look at what’s going on.\nThis is a quick guide on how to get shell console access into a running Docker container.\nUPDATE. If you are running docker version 1.3 or greater then you should use docker exec instead (see docker man pages for more info). The only use case that still requires the use of nsenter is where the user assigned to the container prevents you from doing tasks inside. e.g. where root access is needed but the container is not running as root.\nFirst you need to ensure that nsenter is installed on the host server. Nsenter allows us to enter the Linux namespace that a Docker container is running in.\nOn my Ubuntu server I couldn’t find nsenter in the package manager (maybe util-linux is too old?) so I built it from source.\nSee my other blog post for instructions on compiling nsenter from source.\nFirst, we need to get the process id of the running container. This can be obtained by running docker inspect against the container we want to access. e.g.\nsudo docker inspect mycontainer \"State\": { \"ExitCode\": 0, \"FinishedAt\": \"0001-01-01T00:00:00Z\", \"Paused\": false, \"Pid\": 4614, \"Running\": true, \"StartedAt\": \"2014-08-04T20:14:30.785206674Z\" } Note the Pid field in the State section (4614 in the example above). sudo nsenter -m -u -p -n -i -t {PID}\nWhere {PID} is the process ID taken from the above docker inspect output. e.g. sudo nsenter -m -u -p -n -i -t 4614\nYou should now have a shell inside the container, and be able to stop \u0026 start programs, check out the filesystem, permissions and even install extra applications. Just remember that if you kill the program that the docker RUN command started then the container will exit.\nWhen you’re done snooping around the container, just type exit or Ctrl-d to exit from the container. The container will continue to run and any changes you made within the container will still be in there. Cool or what?\n","tags":["docker","nsenter"],"title":"Console access into a running Docker container","uri":"/console-access-into-a-running-docker-container/"},{"categories":["Docker","Linux"],"content":"nsenter is a great command line tool for accessing docker containers. Unfortunately it isn’t available in Ubuntu 14.04 at the time of writing. Fortunately building it from source is quite simple.\nGet the latest version of util-linux from kernel.org https://www.kernel.org/pub/linux/utils/util-linux/\nAt the time of writing the latest version was v2.25, which is the version used in the examples here.\nInstall the build dependencies sudo apt-get install build-essential libncurses5-dev libslang2-dev gettext zlib1g-dev \\ libselinux1-dev debhelper lsb-release pkg-config po-debconf autoconf \\ automake autopoint libtool python2.7-dev\nDownload the util-linux package source code (this contains nsenter) cd /tmp wget https://www.kernel.org/pub/linux/utils/util-linux/v2.25/util-linux-2.25.tar.gz tar -xvf util-linux-2.25.tar.gz\nNow we’ll compile the nsenter program cd util-linux-2.25 ./configure make nsenter sudo cp nsenter /usr/local/bin\nConfirm that nsenter in installed nsenter --version\nshould return: nsenter from util-linux 2.25\n","tags":["docker","nsenter"],"title":"Install nsenter from source","uri":"/install-nsenter-from-source/"},{"categories":["Linux","Commentry"],"content":"Like most Linux users, I appreciate the power and flexibility of open source and like that my development PC runs the same software as my servers. While building a desktop PC and installing Ubuntu on it is great n’ all, I wanted my next Linux desktop to be a little more portable. Coding in the garden anyone? yes please!\nNow, I really like the look of Apple’s macbook air. But apple’s wonder machine is out of my budget and I’d also have had to switch from Linux to MacOX.\nSo, after a bit of research and a bit of time on eBay, I found myself to be the owner of a used Acre Aspire S3 UltraBook. All in, cost me less than a quarter of the price of a macbook air.\nThe Aspire came pre-installed with Windows, but my plan was to replace that with Ubuntu 14.04 (the lastest Ubuntu at the time of writing).\nThe aspire has no optical media, so the first thing is to copy the Ubuntu installation image onto a USB stick, and install from that. Note that you can’t just copy the .iso file onto the USB, as that won’t boot.\nI have a Linux desktop already, so I simply downloaded Ubuntu onto that and copied it to a USB stick using the dd command: sudo dd if=/path/to/ubuntu-14.04-desktop-amd64.iso of=/dev/sdX bs=4096\nWhere /dev/sdX is the USB device. Please be careful and tripple check you are entering the correct device (dmesg can help by telling that you just plugged in a USB device). If you accidentally overwrite your root hard drive, don’t blame me!\nNext plug the USB into the S3 and switch on. I strongly recommend running on mains power while installing Ubuntu.\nTo boot from USB, you’ll need to press F2 when at startup and either change the boot order, or enable the boot select menu option.\nOnce Ubuntu has booted up, the installation is pretty standard. The only issue I had was that the installer does not seem to have an option to connect to a hidden wifi to allow it to install updates as it goes. However, this did not cause any problems as an internet connection is not actually need during install and I was easily able to update once the install had completed.\nOnce the install had completed the S3 would not boot correctly. It dropped into the busybox command shell with an error saying that the root disk could not be found. Interestingly it all looks to be where it should, and if I typed exit from busybox Ubunto continued to boot and loaded the desktop.\nAfter a bit of experimentation with rootdelay and rootwait, which didn’t make any difference, I found that the issue was with using disk uuid was causing the boot problem (I’m still not sure why though). To fix the problem edit the default grub boot config: sudo vi /etc/default/grub\nand uncomment the line : GRUB_DISABLE_LINUX_UUID=true\nwhile we’re editing the grub config, we might as well make the screen backlight buttons work correctly, as by default they look like they work, but don’t actually change the screen brightness.\nFind the line that starts GRUB_CMDLINE_LINUX_DEFAULT= and change it so it looks like this: GRUB_CMDLINE_LINUX_DEFAULT=\"quiet splash acpi_osi=Linux acpi_backlight=vendor\"\nThen save the file and run grub-update to make the changes take effect at system start. sudo update-grub\nThe only problem left is that the screen brightness always starts at 100%, meaning that you’d have to manually reduce it every time you started the system. The following line will change the brightness to a specific value on each reboot.\nEdit /etc/rc/local and add the following line, just before the exit 0 line at the bottom of the file. intel_backlight 20\nThe above will set the brightness to 20%, just change the number to whatever you want it to be. Note that setting it to 0 will make the screen black and unreadable.\nThe only niggle I did find was that when resuming from suspend (i.e. close the lid to suspend, then open it to resume) the screen would be black with just the cursor showing. At first I thought the machine had locked up because the cursor didn’t move, but it turns out that it is just slow to resume and did come back eventually.\nI found it easier if I turned screen lock off from the brightness and lock setting in system settings. Then I can just open the lid and wait maybe 30 seconds for the desktop to appear. The downside to this is that I don’t have to enter my password when resuming, but this isn’t a problem for me.\nAnother issue I found is that sometimes Wifi does not re-connect after resume. Restarting network-manger quickly restores connection. sudo restart network-manager\nFor most of the time the system is totally silent. The internal fan does come on when when the system is having to do some work, but when web-browsing and text editing I hardly ever hear it start.\nBattery life is somewhat shorter that the 6 hours advertised, typically I get 3:30 hours. This might be because the battery is not new and past its prime though.\nWith the above changes applied Ubuntu runs really nicely on the Acer Aspire S3. So far everything seems to work, including hardware accelerated 3D. The multi-touch touchpad works out of the box.\nAll in all, a really nice Linux laptop at a bargain price. What’s not to like?\n","tags":["acer","aspire","laptop","ubuntu"],"title":"Acer Aspire S3 UltraBook and Ubuntu 14.04","uri":"/acer-aspire-s3-ultrabook-and-ubuntu-14-04/"},{"categories":["Linux"],"content":"Using an SSH tunnel is a great way to administer remote services without having to directly expose them to the internet. You basically forward a port from your local machine (e.g. your Linux desktop) to a port on the remote server. You can then connect to the port on localhost and the magic of ssh will forward the port securely to the remote machine. To the remote server you’ve connected locally on localhost!\nI wanted to run pgAdminIII on my local machine and connect to my remote Postgres server, so I decided that using an ssh tunnel would be safer alternative than allowing postgres connections through my server firewall. To set up a simple tunnel you can run something like the following on your client (e.g. desktop) PC: ssh -L 2222:localhost:5432 user@server-ip\nThe above will bind to port 2222 locally, and appear as port 5432 on the remote server. The localhost bit between the two port numbers is how the connection will appear on the remote server. The last bit, user@server-ip, is the regular user connection used to ssh to the server. This is the local Linux user that will make the connection locally on the remote server.\nOnce the above tunnel is set up you would connect to localhost on port 2222. On the remote server the user will connect to port 5432, the default postgres listening port.\nFor many setups the above tunnel is enough to allow you to work remotely, and securely. But for some you will need to ensure that PostgreSQL is configured to allow local network connections and that it will authenticate users as specified on the connection.\nFor example, if I connect as user fred, but the database user/role I need to use is called bob, then I need to specify user bob on the database login. By default Postgres won’t allow this, and expects the Linux user bob, not fred, to connect.\nTo fix this we can add a couple of lines to the postgres config file: pg_hba.conf\nThis can be a tricky file to find, on my Centos 6 server it is located in the directory /var/lib/pgsql/9.3/data (note you will need to change 9.3 to the version you are running).\nSo, open pg_hba.conf and then add the following lines: host all all localhost md5 local all all md5\nWhat the two lines above do is allow any user to connect to any database on localhost, or via a local socket connection using password authentication.\nFor the above config changes to take effect we need to restart postgres. service postgresql-9.3 restart\nNow we can test our connection. First we can test that we have a working user on the remote server by running the following command using directly on the remote server. Note you should run this command using any user except the user postgres (the Linux user postgres should be able to connect even without the above changes). psql -h localhost -p 5432 -U postgres\nIf that worked, then we can test via the SSH tunnel. On the client machine (the one you made the ssh tunnel from), open a new terminal and run the following command: psql -h localhost -p 2222 -U postgres\nRemember, port 2222 is the local port we defined that will pop out on the remote server as port 5432.\nNow any time you want to connect to your remote postgres server, simply start an ssh tunnel, and connect locally using port 2222. So for my use of pgAdminIII I simply enter localhost and port 2222 on the admin panel!\n","tags":["postgres","postgresql","ssh"],"title":"Connect to PostgreSQL using an SSH tunnel","uri":"/connect-to-postgresql-using-an-ssh-tunnel/"},{"categories":["Cloud","Linux"],"content":"If you do a fair amount of tinkering on cloud servers, such as EC2 or Digital Ocean, then you’ll have noticed that each time you start a server it gets a new IP address allocated. Then when you next log on to the server via ssh the server gets added to your known_hosts file.\nBefore long your known_host file has dozens, if not hundreds of entries for servers that you’ve long shut down and discarded.\nAt first I would edit the known host file and remove the server entry once I’d done with the server, but this got tiresome after a while. So, I decided that for the servers I’m just messing about on I’d rather not add the server into the known_hosts file at all.\nWarning! Before proceeding you should be aware that the reason the remote server is added to the known host file is to prevent man-in-the-middle attacks. So preventing the entry being added will compromise your security. This is why I only do this for the temporary servers that know I’ll be deleting shortly. I suggest you do the same!\nTo prevent the server being added to the known_hosts file we can simply tell ssh to use a different known_hosts file. I use the black hole /dev/null as in the example below. ssh -q -o UserKnownHostsFile=/dev/null user@hostname/IP-address\nThe next problem to overcome is that everytime you connect ssh will warn you that the server identity cannot be verified and you have to type ‘yes’ to continue. This is because the server is not listed in your known_host file and so ssh acts at though every connection is the first connection. To fix this we can switch off host checking (again, another security compromise!). The example below shows the ssh command with both options set. The -q option just stops ssh from throwing out warning messages. ssh -q -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no user@hostname/IP-address\n","title":"How to stop ssh from adding a server to known_hosts file","uri":"/how-to-stop-ssh-from-adding-a-server-to-known_hosts-file/"},{"categories":["Cloud","Linux"],"content":"Using Bitbucket from a network that does not allow ssh (port 22) connections means that doing a git push or pull using the standard git ssh remote won’t work. git remote add origin https://USERNAME@bitbucket.org/USERNAME/REPOSITORY.git\nWhere USERNAME is your user/login name and REPOSITORY is the name of the repository you want to access.\nIf you don’t want to enter your password every time you connect to the remote you can include the password in the remote url: git remote add origin https://USERNAME:PASSWORD@bitbucket.org/USERNAME/REPOSITORY.git\nBut note that the remote URL, and thus your password, is shown on screen when you run git push etc.\n","tags":["Bitbucket","Git"],"title":"Add a GIT remote as https instead of ssh","uri":"/add-a-git-remote-as-https-instead-of-ssh/"},{"categories":["Cloud","Linux"],"content":"I notice that not all cloud hosted Linux servers have a swap enabled. When I spin up a new Centos based server on Digital Ocean for example, there is no swap space.\nUsually you’d use a dedicated disk partition for swap, but when you only have the one disk allocated to your cloud server this is not an option. A good alternative is to create and use a file for your swap space. Here’s a quick guide on how to do it.\nBefore we start, make sure you’re logged in as root.\nFirst, let’s confirm that there is indeed no swap space. We’ll do this by using the free command: \u003cpre\u003efree -m\noutput: total used free shared buffers cached Mem: 499 47 451 0 4 23 -/+ buffers/cache: 19 480 Swap: 0 0 0\u003c/pre\u003e Note that swap is zero, confirming that there is no swap space in use on this server.\nNext, we’ll create a file of the size that we want our swap to be. I know that there are many many opinions on how big swap should be, but I find for a small could server a good option is to make it the same size as the amount of RAM. So, my server has about half a gig of ram, so that’s the size of the swap file I’ll create here.\nThere are a couple of ways to create an empty file of a specific size. Good old dd is my favourite, but it can be a bit slow if the file is large. Alternatively you can use fallocate which is much faster as it does not write data to the disk like dd does. For example, creating a 512 MB swap file with dd: dd if=/dev/zero of=/swapfile bs=1M count=512\nor fallocate: fallocate -l 512M /swapfile Once the file is created, we must change its permissions so that it it not world readable:\nchmod 600 /swapfile Next, format the file as a swap file: mkswap /swapfile example output: /swapfile: warning: don't erase bootbits sectors on whole disk. Use -f to force. Setting up swapspace version 1, size = 524284 KiB no label, UUID=67ab43a1-d567-4275-abe1-09d190dd0d39 Now activate our new swapfile: swapon /swapfile\nRun the free command again to check that its all working: free -m output: total used free shared buffers cached Mem: 499 491 7 0 2 463 -/+ buffers/cache: 26 472 Swap: 511 0 511 To make sure that the swapfile is used after a server reboot we need to add an entry for it to /etc/fstab: Edit \u003ccode\u003e/etc/fstab and add the following line to the end of the file: /swapfile none swap defaults 0 0 And that’s it! Your server now has some Linux swap space.\n","description":"How to add a swap file (not partition) to linux","tags":["swap"],"title":"Add a Linux swap file","uri":"/add-a-linux-swap-file/"},{"categories":["Linux"],"content":"A quick’ cut and paste’ tip for setting the timezone on Centos / Red Hat Linux.\nTo set it for the UK, as the root user, enter the following: ln -sf /usr/share/zoneinfo/Europe/London /etc/localtime All of the different timezones can be found in /usr/share/zoneinfo under various subdirectories, e.g. Europe, Africa etc. To set the appropriate timezone just replace the symbolic link at /etc/localtime to point the the relevant timezone file. In the example above the symbolic link, /etc/localtime is updated to point to the London timezone file.\n","tags":["centos"],"title":"How to set Centos to UK timezone","uri":"/how-to-set-centos-to-uk-timezone/"},{"categories":["Cloud","Linux"],"content":"I recently started using the excellent digital ocean for some of my cloud server hosting. When logging into my servers there was quite a delay, often a minute or more, before the password prompt appeared. I was getting the same delay when I use SSH keys for authentication too.\nI’ve had to solve this problem before on several occasions, but I always end up re-investigating the cause. So this time I thought it was about time I made a note of how to fix the problem.\nRight! First off let’s see where the delay is by logging on the the server with the ssh debugging enabled. This is done by using the -v switch: ssh -v root@server-ip-address\nThis shows conversation that the ssh client is having with the server as it negotiates a suitable authentication method.\nFor me, whenever I see long pauses in the login it’s always because ssh hangs whenever it checks for GSSAPI. Apparently GSSAPI stands for Generic Security Service Application Program Interface.\nSo, the ssh debug info generally scrolls by pretty swiftly until it gets to:\ndebug1: Next authentication method: gssapi-with-mic Then ssh just hangs for a while before reporting something along the lines of debug1: Unspecified GSS failure The fix is to simple disable GSSAPI as an ssh authentication option. This is done by editing the ssh server configuration file: /etc/ssh/sshd_config\nSearch for the line: GSSAPIAuthentication yes and change yes to no; i.e.: GSSAPIAuthentication no After applying and saving the above change you’ll need to restart the sshd daemon. On most systems this can be done by typing: service sshd restart\nNow next time you connect to your server with ssh, it should all happen much faster.\nUpdate – Alternative (client side work-around)\nAn alternative to fixing the server side, you also switch GSSAPIAuthentication off as an authentication option every time you connect. Note that this will only affect that single connection, so you’ll need to specifiy the option every time you connect. ssh -o GSSAPIAuthentication=no root@server-ip-address\n","tags":["login","server","ssh"],"title":"How to speed up a slow SSH login","uri":"/how-to-speed-up-a-slow-ssh-login/"},{"categories":["Linux","Programming","Commentry"],"content":"By far the most popular source control system in use today is Git. It was only about 6 months ago I made the switch from subversion for my personal code projects, and the only regret I have is that I didn’t do it earlier!\nI’m lucky enough to experience code management at both ends of the spectrum. In my spare time I tinker with personal projects (some public but mostly private stuff). At work ‘enterprise’ would definitely be an appropriate description of our source code management.\nWith the advent of cloud computing, source control can no longer be kept neatly behind corporate firewalls.Of course you could stick a Subversion server or Git repository in your DMZ, but really, the correct answer is hosting.\nThere are many source-code hosting providers to choose from, but in this post I’m going to look at the two most popular Git hosting solutions: Github and Bitbucket.\nBoth Github and Bitbucket offer public and private repositories, wiki and issue tracking. So which one is best? Well, like most things in IT (and in life too I guess), it depends…\nEven though both offer very similar same feature sets the main deciding factor will be based on whether you’re looking for public or private repository hosting.\nGithub is by far the most popular of the two with around 4 million users compared to Bitbucket with 1 million. Github currently has a much stronger community than Bitbucket, and that community is all about collaborative coding and open source projects. The Github slogan sums it up quite nicely: social coding.\nBoth Bitbucket and Github offer unlimited public repositories for free, but if you need a home for your open source project code, you really should look no further than Github. That’s where everyone else hosts their open source code, and so should you.\nSo while public repositories are free, it’s the different pricing models for private repositories that separate the two, especially when choosing for enterprise hosting.\nBasically the difference is this; Github allows unlimited users, but charge for the number of private repositories you have. Bitbucket allows for unlimited private repositories, but charge for the number of users you have.\nIf you only have a few private repositories then the cost would be pretty similar, although it’s worth noting that the first 5 Bitbucket users are free. So if you’re a small team or fly solo then a free Bitbucket account with unlimited private repositories is a no brainer!\nIt seems to me that Bitbucket was designed with a bias towards business and enterprise users rather than open source developers. It can integrate with other Altassian products, specifically Jira and Confluence, Altassian’s excellent enterprise issue tracking and collaboration software. But even without these add-ons Bitbucket just feels somehow more professional than Github.\nSome PAS (Platform as a Service) solutions such as Heroku currently integrate better with Github, but I think this will change as enterprises continue to adopt and turn to the cloud. It isn’t difficult to use Bitbucket with Heroku, it’s just Git after all, but you can link your github account to Heroku which can simplify things a bit.\nIn the end your choice will likely come down to cost. If you need private repositories then you’ll need to work out the cost of each platform based on the number of private repositories you’ll need and the number of developers/users that will need access.\nPersonally I have found that the number of private repositories we need continues to rise, but the number of developers/users less so. This makes Bitbucket a more cost effective option in our case. I realise that this is more of a preference, but I do prefer the more business-like feel of Bitbucket when it comes to hosting private projects.\nTL;DR Both Github and Bitbucket are excellent Git repository hosting solutions, and really the best choice is to use them both. For Open Source I use Github every time. When I need private repositories Bitbucket is my tool of choice.\nSometimes however, I still use Github for private repositories, but this is mainly when a PAS integrates better with GitHub than with Bitbucket (This is getting much less frequent as the popularity of Bitbucket grows).\nOne thing is clear though. If you’re not using Git for source and version control then I urge you to seriously consider switching. As PAS (Platform as a Service) becomes ever more pervasive, Git is becoming a pre-requisite for code deployment to the Cloud. And besides, once you switch you might just discover why Git is the most popular version control system on the planet.\n","tags":["Bitbucket","Git","Github","source-code","Subversion"],"title":"Github vs Bitbucket","uri":"/github-vs-bitbucket/"},{"categories":["Linux"],"content":"Stud is the “Scalable TLS Unwrapping Daemon” written by bumptech and used by 85million bump users. I use it to offload SSL connections in front of haproxy for really scalable websites. It’s not so much that I need to scale massively, but it does allow me to run on really small (i.e. cheap) servers. I’m aware that the development branch of HAproxy now supports SSL directly, but as yet I have not seen any performance tests so for now I will stick with stud.\nI have previously built stud on Debian Linux and the build process is really simple; make and then make install (once openssl-devel in installed and or course the utilities gcc and make too!)\nBut I’ve recently moved over to Centos as my server OS of choice and ran into some problems whilst building stud. The error reported when I ran make was file ev.h not found.\nSo, I used yum to install libevent/libevent-devel, but I still got the same error! It turns out that libev and libevent are not the same thing!\nI had to manually download and install libev/libev-devel because they are not in the yum repository.\nThe versions I installed can be downloaded from here:\nhttp://dl.fedoraproject.org/pub/epel/6/x86_64/libev-4.03-3.el6.x86_64.rpm http://dl.fedoraproject.org/pub/epel/6/x86_64/libev-devel-4.03-3.el6.x86_64.rpm\nThe above libs are for Centos 6 64 bit (I’m running Centos 6.4 64bit). Other versions can be found on the same site, http://pkgs.org/\nI installed tham using the yum command: yum localinstall libev-devel-4.03-3.el6.x86_64.rpm libev-4.03-3.el6.x86_64.rpm\u003c\nEven after installing libev the make still failed. It seems that the header files were not installed to the correct place.\nI simply copied the following file into the common libs folder: cp /usr/include/libev/ev.h /usr/include\u003cbr /\u003e\nNow, finally, I was able to make and install stud!\n","tags":["centos","stud"],"title":"Compiling bumptech Stud on Centos 6","uri":"/compiling-bumptech-stud-on-centos-6/"},{"categories":["Linux","Commentry"],"content":"My Humax freeview recorder (a HDR-FOX T2 with 1tb drive) has recently started to show signs that it might be about to fail. The Hard drive is getting louder when recording and my most recent recordings pause periodically, often with the screen going blank and displaying the message ‘encrypted video cannot be show’ (or something to that effect). At first the pauses were infrequent and short, but more recently they’ve been getting much worse often missing a minute or more of the recording per ‘glitch’. The hard drive test found in the setting menu is also failing now too.\nSo, I emailed Humax and after a week without a reply I thought I’d give them a call…\nI was expecting a replacement, or at least a repair, but no. It seems that the problem could be tuning (really?!?) or I might need to format the hard drive. Only when I’ve done both of these things will a replacement be considered.\nOf course this does highlight a problem with consumer hard drive recorders. If the drive fails you lose all your recordings even with a replacement unit. I’m now regretting replacing my RAID 1 mythtv setup…\nSo, formatting the drive will obviously wipe out all of my recordings. So I hooked up a USB hard disk and started copying the content off the humax. For SD recordings they are decrypted as they copy over to the USB drive. Unfortunately HD recordings don’t get decrypted. Some sort of DRM fluff n nonsense I think.\nNow, I don’t think this would be a problem if I copy them back onto the humax for viewing, but if the format does not work and the drive is faulty then copying them onto a new humax wont work. Bah!\nThere is a Windows program called foxy that can be used to modify the binary metadata file of a HD recording so that it will be decrypted when copied to a USB drive (just like SD recordings are). But I could not get this to work on Linux.\nI intend to write a simple(ish) program to do the same thing on Linux as this sounds like a useful backup tool to have. But I’m not sure if I’m going to stick with the Humax. It just depends if the drive is faulty or not. My thinking is that if the drive is faulty then I’ll get a replacement and that’s that. But if formatting the drive does fix the problem then this highlights a major issue for me. I don’t fancy having to ‘wipe’ my recording once or twice a year just to keep the thing going!\nHere is how I removed the encrypted flag manually using Ubuntu Linux.\nBIG FAT WARNING Editing files could make your recordings unplayable, and could even invalidate your warranty. I’m not even sure if backing up HD content is even allowed. The instructions below are simply a blog of what I did to my humax files. There are no guarantees that they will work on your humax files. So if you follow these instructions and bad things happen to your humax recorder and/or recordings, don’t blame me! Remember, the risk of corrupting or damaging your humax hard drive recorder is yours not mine. END OF BIG FAT WARNING\nMake sure you can FTP to your Humax box (RTFM on how to enable FTP). Hint: the default username is humaxftp and the password is 0000 which is your pin in case you’ve changed it There are four files for each recording. The one we’re interested in has the extension .hmt Copy the .hmt file to your PC. Then simply use a hex editor to modify the file. I suggest you make a backup copy of the file before editing it. The byte we need to change is at offset 0x3DC and will contain either 00 or 02 if the recording is encrypted (actually all files are encrypted, I guess the flag really means decryption allowed). Using the hex editor change this to 04 and then save the file. Now copy the file back to the humax box. When you navigate to the recording on the humax box you should no longer see ‘enc’ next to the HD symbol. Thanks to myhumax.org for information on hmt files. The hex editor I used is called Bless Hex Editor.\n","tags":["humax","pvr"],"title":"Copy Humax HDR-FOX T2 HD recording to USB","uri":"/humax-fox-t2-hd-copy/"},{"categories":["Linux"],"content":"I’ve always had trouble downloading footage from my old DV camcorder using desktop tools like Kino, with it often locking up when I leave it to download a full tape.\nI prefer to use OpenShot for editing, so all I really need is a simple tool to allow me to download footage from the camcorder. I’ve recently discovered the command line tool dvgrab, and it works wonderfully!\nOnce installed (apt-get install dvgrab on debian/ubuntu) the following command will rewind the tape and start to import footage. A new file will be created for each ‘break’ on the tape. The files are written with the recordings date/time as the filename and a prefix as specified on the command. Just remember to cd to the folder you want to have the files written to before running the command.\ndvgrab -a -format raw -rewind -t holiday-\nWhere holiday- should be replaced by whatever you want to have as a prefix on each filename.\n","tags":[1394,"camcorder","dvgrab","firewire"],"title":"Downloading from DV firewire 1394 camcorder with dvgrab","uri":"/dv-firewire-1394-camcorder-with-dvgrab/"},{"categories":["Linux"],"content":"I recently set up HA proxy on one of my Debian 6 (squeeze) servers.\nThere were a few rather annoying ‘gotchas’ that took a while to figure out, so I’m documenting them in case I ever have to set up HAproxy again.\nThe basic install was done using apt-get install haproxy\nThis does create all the init scripts needed, but rather annoyingly they won’t work until you change the config file /etc/default/haproxy and change the ENABLED variable to ENABLED=1\nNext up is logging. Haproxy logs via rsyslog, which needs to be configured.\nFirst, create a folder to hold the logfiles: mkdir /var/log/haproxy\nIn the haproxy config, we’ll tell it to log to local0, add to /etc/haproxy/haproxy.cfg:\nglobal\u003cbr /\u003e log /dev/log local0 info\u003cbr /\u003e log /dev/log local0 notice\u003cbr /\u003e (see below for a full example of haproxy.cfg)\nNext, we will configure rsyslog to catch the logs and write to the log files. Thanks to debuntu.org for this information.\nCreate a file /etc/rsyslog.d/haproxy.conf and insert the following code into it:\nif ($programname == 'haproxy' and $syslogseverity-text == 'info') then -/var/log/haproxy/haproxy-info.log\u003cbr /\u003e \u0026 ~\u003cbr /\u003e if ($programname == 'haproxy' and $syslogseverity-text == 'notice') then -/var/log/haproxy/haproxy-notice.log\u003cbr /\u003e \u0026 ~\nNow we’ll configure logrotate to keep our logfiles in some kind of order. create a file /etc/logrotate.d/haproxy and insert the following code into it:\n/var/log/haproxy/*.log {\u003cbr /\u003e weekly\u003cbr /\u003e missingok\u003cbr /\u003e rotate 7\u003cbr /\u003e compress\u003cbr /\u003e delaycompress\u003cbr /\u003e notifempty\u003cbr /\u003e create 640 root adm\u003cbr /\u003e sharedscripts\u003cbr /\u003e postrotate\u003cbr /\u003e /etc/init.d/haproxy reload \u003e /dev/null\u003cbr /\u003e endscript\u003cbr /\u003e }\nThis will keep seven weeks of logs.\nAnd finally we can configure the load balancer itself. This is really specific to your requirements, so the official documentation is a good place to start here. However, a very simple example is shown below.\n`\nListen on port 80 and rout all traffic to localhost port 20002 except the usl is /echo/ then route to port 20001` global\u003cbr /\u003e log /dev/log local0 info\u003cbr /\u003e log /dev/log local0 notice\u003cbr /\u003e maxconn 4096\u003cbr /\u003e user haproxy\u003cbr /\u003e group haproxy\u003cbr /\u003e daemon\ndefaults\u003cbr /\u003e log global\u003cbr /\u003e mode http\u003cbr /\u003e option httplog\u003cbr /\u003e option dontlognull\u003cbr /\u003e retries 3\u003cbr /\u003e option redispatch\u003cbr /\u003e maxconn 2000\u003cbr /\u003e contimeout 5000\u003cbr /\u003e clitimeout 50000\u003cbr /\u003e srvtimeout 50000\nbackend hello\nserver hellosvr 127.0.0.1:20002\nbackend echo\nserver echosvr 127.0.0.1:20001\nfrontend http_in\nbind *:80\ndefault_backend hello\nacl rec_echo path_beg /echo/\nuse_backend echo if rec_echo\nerrorfile 400 /etc/haproxy/errors/400.http\u003cbr /\u003e errorfile 403 /etc/haproxy/errors/403.http\u003cbr /\u003e errorfile 408 /etc/haproxy/errors/408.http\u003cbr /\u003e errorfile 500 /etc/haproxy/errors/500.http\u003cbr /\u003e errorfile 502 /etc/haproxy/errors/502.http\u003cbr /\u003e errorfile 503 /etc/haproxy/errors/503.http\u003cbr /\u003e errorfile 504 /etc/haproxy/errors/504.http\u003cbr /\u003e\n","tags":["haproxy","proxy","router"],"title":"HAPROXY quick setup","uri":"/haproxy-quick-setup/"},{"categories":["Linux","Node.js"],"content":"This is a quick how to with the steps used to compile Node.js on Centos Linux.\nThese steps were successfully executed on Centos 6.2 x86 with node 0.6.18\nFirst, download the sourcecode from www.nodejs.org and extract it:\ntar -xvf node-v0.6.18.tar.gz\nNext, if you don’t have the development tools already installed you’ll need to install them (as root):\nyum groupinstall 'Development Tools'\n…and you should also install openssl too (as root):\nyum install openssl-devel\nNow we can start the build process.\n./configure\nIf all loks well, lets compile node.js:\nmake\nAnd finally to install it into system, as root run:\nmake install\nIf all that went well you should have a working node.js installation. Type node --version to see if it works.\n","tags":["build","centos","compile","Node.js"],"title":"Compile node.js on Centos Linux","uri":"/compile-node-js-on-centos-linux/"},{"categories":["Linux","Node.js"],"content":"Building Node.js on Linux is fairly simple, as there are few dependencies. This is how I did it on Debain 6 (Squeeze). At the time of writing the current release of Node.js was 0.6.15. You’ll need to adjust the intructions below to reflect the version you’re building.\nUpdate: I have also successfully tested these instructions with node.js 0.6.17 on Ubuntu 12.04.\nDownload the Node.js source, e.g. wget http://nodejs.org/dist/v0.6.15/node-v0.6.15.tar.gz\nExtract the files: tar -xvf node-v0.6.15.tar.gz\ncd into the newly extracted node.js folder: cd node-v0.6.15\nIf you don’t have the essential build tools install then you’ll need to install them. As root run apt-get install build-essential\nYou’ll also need python, if if you don’t already have it installed, as root run apt-get install python\nWe’ll also need to build in SSL support, so let’s install the openssl libs: as root run apt-get install libssl-dev\nAt this point we can run ./configure and proceed with the node.js build. You’ll see that configure will report that openssl is not found, but the build will still work. This is because configure uses pks-config to check for openssl. If the this bothers you then run (as root) apt-get install pkg-config and ./configure will then find openssl.\nRight, that’s the pre-compile stuff done, lets get on and compile node.js! From this point it’s all standard Linux build commands: ./configure, make and make install. So…\nrun ./configure (can be either root or a regular user)\nThen make (again, either as root or a regular user)\nand finally, as root run make install to install node into the system folders.\nAll done, so lets just check Node.js has been successfully installed. Type node --version\nNow all you need to do is start writing some event driven JavaScript code… Over to you!\n","tags":["build","compile","debian","Node.js"],"title":"Compile Node.js on Debian and Ubuntu Linux","uri":"/compile-node-js-on-debian-linux/"},{"categories":["Linux"],"content":"I needed to extract/generate a public ssh key from a java keystore so that the Java application could SFTP some files using public key authentication.\nThe problem was that I couldn’t find any way of converting an ssl public key to an ssh public key. It seems that although ssl and ssh private keys are compatible, the public keys are not.\nNo problem I thought, I’ll just generate the ssh key from the private key held within the java keystore (jks file). Hmm, it seems that the keytool utility does not have such ability, nor will it allow you to extract the private key.\nOkay, so a quick google search later I discover a couple of freeware tools that can extract the private key from a java keystore. Great! I thought… until I found that our company wasn’t too keen on using freeware tools to support a production process… Bah!\nIt took me a fair amount of head scratching and experimentation to figure out a method that uses only the keytool utility and ssh-keygen. Both ship with most Linux distros (keytool also ships with Java), so no-one should object to using them, should they?\nAnyway, here are the commands I used to generate an ssh public key from a private ssl key held within a java keystore.\nConvert keystore from jks to pkcs12 format:\nkeytool -v -importkeystore -srckeystore keystore.jks -srcalias MYKEY -srcstorepass MY_STORE_PASSWORD -srckeypass MY_KEY_PASSWORD -destkeystore keystore.p12 -destalias MYKEY -deststorepass MY_STORE_PASSWORD -destkeypass MY_KEY_PASSWORD -deststoretype PKCS12\nExtract private key from pkcs12 keystore:\nopenssl pkcs12 -in keystore.p12 -out key.pem -passin pass:MY_STORE_PASSWORD -passout pass:MY_STORE_PASSWORD\nSet permissions so ssh-keygen won’t balk:\nchmod 600 key.pem\nExtract public key in ssh format:\nssh-keygen -P MY_STORE_PASSWORD -y -f key.pem \u003e sshkey.pub\nCheck the public key!\ncat sshkey.pub\n","tags":["keystore","ssh","ssl","java"],"title":"Extracting SSH keys from a Java keystore (jks) file","uri":"/extracting-ssh-keys-from-a-java-keystore-jks-file/"},{"categories":["Linux"],"content":"Like the fool I am, whilst messing about with user privileges in phpMyAdmin I managed to delete the root account. Suddenly I found myself with no access to any of my databases It took a while to figure out how to re-create the root@localhost user, so here’s how I did it.\nShut down mysql server Start the msql server up with skip-grant-tables option Log in to mysql Change to the mysql database Create the root user Re-start mysql server For those that like a cut n paste approach, these are the commands: service mysqld stop mysqld_safe --skip-grant-tables \u0026 mysql use mysql create user root@localhost; GRANT ALL PRIVILEGES ON *.* TO 'root'@'localhost' with grant option; commit; FLUSH PRIVILEGES; exit service mysqld restart\nYou can confirm that the root account has been created (or is indeed missing!) by listing entries on the user table:\nuse mysql; select Host,User from user;\nGood luck!\nKev.\n","tags":["mysql"],"title":"Deleted root@localhost account in MySQL","uri":"/deleted-rootlocalhost-account-in-mysql/"},{"categories":["Linux"],"content":"I recently gave K9COPY a try, it’s supposed to be the Linux equivalent of DVDshrink for Windows. It certainly looked the part, but when I ran it with a disk I wanted to back up it ran for a short while and then crashed Looking at the output of dmesg revealed that the disk was having trouble reading the encrypted disk:\n`[235.097347] sr 5:0:0:0: [sr0] Add. Sense: Read of scrambled sector without authentication\n`\nIt was at this point I realised that I had not yet installed the restricted media packages (also needed to play DVD films). So…\nsudo apt-get update \u0026\u0026 sudo apt-get install libdvdread4\u003cbr /\u003e sudo /usr/share/doc/libdvdread4/install-css.sh\n“That should do it” I thought! But no luck, it still didn’t work. I searched synaptic and installed some other packages that looked like they should help but still nothing… The disk would not play or back-up.\nIn the end deleted the contents of ~/.dvdcss and amazingly, it worked!\nSo if you’ve followed the Ubuntu instructions and still can’t play or back up a movie DVD then you might want to give this a try. I’m posting it here because it’s not an obvious solution and I’m sure I wont remember it next time I re-install my system.\n","tags":["dvd"],"title":"K9COPY crashing on Ubuntu 10.10","uri":"/k9copy-crashing-on-ubuntu-10-10/"},{"categories":["Xen"],"content":"Today saw the release of Xen 4.1. Major changes include support for greater than 255 CPUs, a new credit scheduler and CPU pools.\nHowever, the most significant change to those that use Xen will be the new XL toolstack which replaces XM/XEND and will ultimately replace xcp’s xapi and libvirt.\nThere’s no need to panic just yet though because XM is still included in Xen 4.1 and sits alongside XL.\nFurther details can be found in the Xen 4.1 release notes:\nhttp://blog.xen.org/index.php/2011/03/25/xen-4-1-releases/\nIt’s good to see Xen moving forward and continuing to converge with Linux mainline kernel. What I’d really like to see though, is a pre-built up to date Xen Distro and a decent web based management front end. Now that would cause sleepless nights over at VMware HQ…\n","tags":["Xen"],"title":"Xen 4.1 released","uri":"/xen-4-1-released/"},{"categories":["Linux"],"content":"I was converting a TV show I’d to DVD format using ffmpeg, but the resulting file had the wrong audio channel. At first I thought it had no audio at all, but then I noticed it had a voice over occasionally explaining what was happening; “so-n-so has just entered the room…”.\nIt appears that ffmpeg had selected the wrong audio stream. For some reason, it had had selected the second audio stream instead of the first. The fix is simple enough, you can manually set the streams to use with the -map option. The command below works well for UK freeview.\nffmpeg -i \"input.mpg\" -y -target pal-dvd -map 0.0:0.0 -map 0.1:0.1 -ac 2 -aspect 16:9 output.mpg\nYou’ll notice there are two -map parameters. The first one, -map 0.0:0.0 is for the video stream (almost always the fist stream). The second one, ** -map 0.1:0.1** maps the second stream (the first audio stream, remember the first stream is the video) to the second stream (1st audio stream) of the output file. This fixes the audio problem… well almost!\nThe next problem I found was that the audio was in mono. This is fixed by telling ffmpeg there are two audio channels wit the parameter -ac 2.\nHow do you know which audio channel to set? The easiest way is to run ffmpeg without the -map option and note the streams it gives in the summary before it starts displaying it’s progress. You can then ctrl-c to quit. Example output shown below:\nDuration: 02:03:57.24, start: 89212.169567, bitrate: 2944 kb/s\nProgram 1\nStream #0.0[0x200]: Video: mpeg2video, yuv420p, 704×576 [PAR 16:11 DAR 16:9], 15000 kb/s, 25 fps, 25 tbr, 90k tbn, 50 tbc\nStream #0.1[0x28a](eng): Audio: mp2, 48000 Hz, 2 channels, s16, 192 kb/s\nStream #0.2[0x294](eng): Audio: mp2, 48000 Hz, 1 channels, s16, 64 kb/s\nStream #0.3[0x401](eng): Subtitle: dvbsub\nStream #0.4[0x87b]: Data: 0x000b\nStream #0.5[0x943]: Data: 0x000b\nSo it you were to see that the stereo audio is stream 0.3 for example, you’d then use -map 0.0:0.0 -map 0.3:0.1 to map it to stream 0.1 of the output file. The first -map is still needed to specify the video, you’ll get an error without it.\n","tags":["dvd","ffmpeg"],"title":"ffmpeg freeview to pal-dvd audio fix","uri":"/ffmpeg-freeview-to-pal-dvd-audio-fix/"},{"categories":["Linux","Xen"],"content":"The XEN developers ad community have been trying fr years to get DOM0 support into the Linux mainline kernel. On January 5th 2011 it finally happened when Linux kernel 2.6.37 was released.\nOkay, so it’s only basic support at this stage, but it does represent a significant milestone in the hypervisors history.\nDOM0 is the first domain (virtual machine) started by the XEN hypervisor and has privileged access to the computer hardware and drivers.\nDOMU support was added to mainline kernel some time ago through the PARAVIRT_OPS API. However until now the DOM0 had to use a specially modified version of the Linux kernel.\nSo what? I hear you ask… Well the hope is that with DOM0 support available in the stock kernel, Linux distributions will start adding XEN virtualization as an option to along side KVM etc. Combined with all the advantages of having the latest kernel in DOM0, such as disk mirroring, really could deliver enterprise class computing in a cheap, open and community driven package. Now there’s a thought…\n","title":"XEN DOM0 support in mainline kernel at last!","uri":"/xen-dom0-in-mainline-kernel/"},{"categories":["Programming","Retro","Games"],"content":"I’ve been reading a lot about HTML5 recently and it looks like the browser could soon become the dominant application platform. So, I thought I’d have a look at the new canvas element and delve into some JavaScript.\nTo get me started I’ve decided to write a (very) simple version of an old classic zx Spectrum game: Jumping Jack.\nAt the time of writing this only works in browsers that support the new canvas html element. So yay for Firefox and boo to Internet Explorer.\nClick the thumbnail below to run the game. You can the right click or something to view the code.\nClick to launch the… erm, ‘game’\nIf you’re going to use my code as a learning reference then you should note that I’m not a JavaScript programmer and, even then I cut a few corners to speed things up. This simple game is the result of an afternoons tinkering, so don’t expect much!\nWhile the canvas element looks great for simple 2d drawing, the html5 feature that really excites me is WebGL. When its finally available it should be a game changer!;\n","description":"A very simple Jumping Jack zx Spectrum remake written in JavaScript for the browser. Written as a programming learning exercise.","tags":["html5","JavaScript"],"title":"Adventures in JavaScript Canvas","uri":"/adventures-in-javascript-canvas/"},{"categories":["Android"],"content":"This short guide takes you through the steps needed to install a custom Android 2.1 ROM onto the original T-Mobile Pulse.\nThe official T-Mobile Android 2.1 ROM was removed from T-Mobiles website because it had a few bugs, the worst of which meant that you’d lose the odd text message. However the latest ‘custom’ ROMS appear to have fixed these problems.\nI upgraded my phone using the FLB 1.7 ROM and various guides over at the Modaco forums (www.modaco.com). This is a simple step by step guide of how I upgraded my phone. You might want to also check with the forums for additional information, especially this thread.\nFirst off, some background information.\nAs far as I know this guide will only work with the original pulse; the Huawei U8220. Take the battery out to check the model number of your phone. The Pulse has a limited amount of memory which is not enough to run Android 2.1 (this was part of the problem with the official t-mobile release). To fix this you need to run a swap partition on your SD memory card. A class 6 SD card is recommended, however I have this working with a class 4 card. I have an 8GB class 4 card in my phone, but you can get away with using a 1GB card, just make sure its a class 4 or higher for it to be fast enough to run swap. BIG FAT WARNING Installing a custom ROM on your phone will almost certainly invalidate your warranty. It is also possible that you could brick your phone. The instructions below are simply a blog of what I did to my phone. There are no guarantees that they will work on your phone. So if you follow these instructions and bad things happen to your phone, don’t blame me! Remember, the risk of making your phone completely useless is yours not mine. END OF BIG FAT WARNING\nThe basic stages we’re going to perform for upgrading are:\nInstall the official T-Mobile 2.1 ROM. This is needed because this upgrade modifies the phones internal partition sizes. The custom ROM won’t work with the original Android 1.5 sizes. Install a custom recovery boot image Install the custom Android 2.1 ROM Configure the ROM to use SWAP and run apps from the SD card. First off, download the software needed onto your PC.\nOfficial T-Mobile Android 2.1 update\nRecovery boot image\nFLB 1.7 Custom ROM (aka custom Android 2.1)\nFixed terminal emulator\nBack up anything you need to keep from your phone. e.g. sync your contacts with google etc. Copy anything you need to keep from your SD card to your PC because your card will be wiped during the upgrade.\nNote: If you have been running a custom 1.5 ROM with A2SD, it’s best to re-partition your SD card back to a single FAT32 partition before you start the upgrade. The easiest way to do this is to put your SD card into a card reader and use Gparted.\nRight, now you’ve backed up everything you need, lets start the upgrade…\nStage1, apply the t-mobile official 2.1 update.\nUnzip the Official 2.1 ROM and copy the DLOAD folder onto the root of your SD card. By root I mean don’t put it in any sub-folder on the SD card. With the SD card back in the phone and the phone powered off, hold down the buttons ‘red end call’ + ‘volume up’ + ‘power’ and wait for the preparing update message. Confirm that you want to update and then wait for the update to complete. At the end of the update you’ll get a message saying the phone will restart. For me the phone just hung. If this happens simply remove and re-insert the the battery Confirm that the 2.1 update has worked, but don’t go mad and install apps etc because you’ll be wiping the phone again in a moment. Stage 2 – Install recovery boot image.\nUnzip the recovery image onto you PC and navigate to the folder. Make the file install-recovery-linux.sh executable (i.e. chmod +x install-recovery-linux.sh) With your phone powered off hold down the buttons ‘red end call’ + ‘volume down’ + ‘power’. The screen should go blue. Connect the USB cable to the phone and PC On the PC as root run install-recovery-linux.sh (e.g. in ubuntu cd to the folder containing the superboot files and type sudo ./install-recovery-linux.sh). When you get the successful message you can reboot the phone (i.e. remove/re-insert the battery. You’ll probably have to remove the USB cable too). Stage3 – Install the custom FLB Android 2.1 ROM\nBoot into the recovery menu by holding down the buttons ‘red end call’ + ‘menu’ + ‘power’ and wait until the recovery menu appears (after 10 seconds or so). Plug the USB cable into the phone and PC. Using the track-ball scroll down to the option ‘USB toggle’ and press the track-ball to select. The SD card should now mount on your Linux PC. Copy the custom rom zip file (flbmod_v1.7.zip) to the root of the sd card. Don’t unzip it, just copy the zip file. Once the zip file has been copied, press the green button to toggle the USB back to off. Use the track-ball and select the ‘Wipe’ option. Choose the First option (Factory reset) and confirm. Once the wipe has completed return to the main menu (with the ‘home’ button). ** **Now select ‘Flash ZIP from SD card’. Select the custom rom you wish to flash, confirm and then wait for the installation to complete. Return to the main menu and select ‘Partition SD card’. Using the ‘volume up/down’ buttons to change the sizes and ‘green’ to confirm set the swap to either 32 or 64mb. I went for 64mb (don’t go any bigger because it will slow the phone down). Then a suitable size for the ext2 partition. The ext2 partition is where the apps will be installed. 512mb is OK, 1024 is better if you have a big enough SD card! Return to the main menu end select the ‘reboot’ option. Be patient! it takes ages for the phone to start up on the first reboot! Stage 4 – Configure the ROM to use the SD card for apps.\nAlthough there are apps to SD (a2sd) options in the recovery menu, you should not use them. They are for an older version of a2sd and will cause problems if used on the version included in the Android 2.1 ROM.\nOpen the terminal emulator app. When I did this the app crashed with a force close. If this happens to you then you’ll need to install the fixed version you downloaded earlier. Copy the file to the FAT32 (data) partition of your SD card (this can be done from the USB option in the recovery menu or in the usual Android fashion) and navigate to it using es file explorer from the installed apps menu. You can unzip and then navigate to …fixterm/system/app and tap on Term.apk and install it. In the terminal emulator app type su - to switch to the root user. Now type a2sd check to list all the current settings. If you scroll through the output you should see that swap is running, but the apps and dalvik cache are on the internal storage. To move the apps to the SD card type a2sd reinstall. Your phone will then reboot to apply the settings (again, the first boot takes a while) To move the dalvik cache to the SD card type a2sd cachesd. Again, your phone will restart to apply the settings. Please note that unless you have a very fast SD card (class 6+) you should leave the cache on the internal memory. If you have the cache on the SD card and need to move it back to the internal storage type a2sd nocache. Warning: There is another setting, a2sd cachepart. DO NOT USE THIS SETTING! it will stop your phone from booting and you’ll have to wipe your phone and start again. That’s it. You should now be able to enjoy Android 2.1 on your Pulse. I recommend that you now boot into the recovery menu and make a nandroid backup of your phone before… Extra.\nAfter using the above upgrade for a short while I noticed my phone would slow down after about 10-15 minutes of heavy use. This appears to be because pages are being moved to/from swap in high volume. To fix this I reduced the swappiness from the default of 60% down to 20% with the command a2sd swappy20 (as root in terminal emulator app) I also set the memory ‘task killer’ setting to moderate with the command a2sd lowmem-moderate. Then manually restarted the phone.\nTip: to check the current value of swappiness type cat /proc/sys/vm/swappiness\n","title":"Installing Android 2.1 onto T-Mobile Pulse","uri":"/installing-android-2-1-onto-t-mobile-pulse/"},{"categories":["Linux"],"content":"If you’ve ever had a collection of mp3 files that you want to burn to CD for playback in the car or archiving etc, you’ll likely know how annoying it can be when the collection of mp3 files is just a bit too big to fit on a CD. You end up having to remove a couple of tracks… While this is probably OK for music (there’s always a couple of tracks I’d skip anyway), it’s not so good for audio books and the like.\nOne answer to the problem is to reduce the size of the mp3 files themselves. Be warned though that\nthis is not without a down side. Shrinking the mp3 will reduce it’s sound quality. This is less likely to be a problem for audio books, but you might not want to do this for music.\nThe program lame is good for this sort of thing. You give it an mp3 file, specify a new (lower) bitrate and an output file and you get a new, smaller mp3.\nlame --mp3input -b 80 input.mp3 output.mp3\nBut what if you have a folder full of files? You could type the above command for each file, but that’s going to be a pain. The simple command below fill loop through a folder of mp3 files converting each one and placing it into a sub folder named converted. To use the command ‘as is’ you’ll need to create the sub folder first and run the command from within the folder that contains your mp3′s.\nfor file in *.mp3 ; do lame --mp3input -b 80 \"$file\" ./converted/\"$file\" ; done\nThe -b option specifies the bit rate. The smaller the number the smaller the file (and lower quality). An average music track tends to be encoded at between 128 and 256, when audio books are generally lower. Obviously you should check what rate the mp3 is before deciding on a new bit rate. In gnome (Ubuntu) this is easy by right clicking the file and selecting ‘properties’, and then the ‘audio’ tab.\n","tags":["lame","mp3"],"title":"Reducing the size of mp3 files","uri":"/reducing-the-size-of-mp3-files/"},{"categories":["Linux"],"content":"Okay, so I often hear people (mainly at work) pronouncing Linux incorrectly, with an emphasis on the letter ‘I’, as you would in the word ‘light’ for example.\nAlthough this may sound more professional it really isn’t. I don’t know why it winds me up, but I do have to bite my tongue to stop myself from correcting people and sounding like the office nerd (which, now I think about it I probably am!).\nSo, to settle the matter once and for all, here is a clip of Linus Torvalds pronouncing Linux just as it should be, with a short ‘i’…\n","tags":["Linus-Torvalds"],"title":"How to pronounce Linux","uri":"/how-to-pronounce-linux/"},{"categories":["MythTV"],"content":"After running my MythTV frontend on an old MicroSoft XBOX for a few years as a ‘proof of concept’ I recently got the go ahead from the Missus to replace it with something a little snappier.\nThe old XBOX did a pretty good job all things considered. But with just 64MB of RAM it was never going to have the most responsive interface in the world, so the time has come to replace it.\nAfter moochas googling for a suitable PC that could live happily in the living room under the telly, I decided to build a fanless mini-itx system in as small a case as I could.\nIn the end I ordered an Asus AT3IONT-I Deluxe motherboard, a M350 case and 2GB of DDR3 RAM. For the storage I decided I didn’t need masses for a MythTV frontend, so an 8GB Compact Flash card would do nicely.\nThe Asus motherboard in it’s ‘Deluxe’ flavour comes supplied with a DC power supply, where the Non-Deluxe version needs an ATX power supply. The advantage of DC power is that the power supply is basically a fanless ‘brick’ that sits outside of the case, rather like a Laptop PSU.\nThe board has a dual core atom 330 processor fitted and a heatsink that covers most of the board. Again, no noisy fans!\nOn the back of the board are outputs for HDMI and VGA. The VGA output is important for me as I’m still stuck in the dark ages with a CRT TV, while the 1080P HDMI will come in handy whenI get round to upgrading to one of those new fangled LCD jobbies.\nAnother piece of hardware needed for the build was a Compact Flash to SATA adapter, so the CF card appears to the system as an 8GB SATA drive. This turned out to be quite a cheap way to go, costing just £5 off ebay. This neat storage solution means that I can remove the CF card and back it up on my desktop PC and also swap the CF card for another when I want to upgrade or re-install, keeping my current installation safe.\nThe Software My MythTV Backend server is based on Ubuntu 9.04, and for now I don’t want to upgrade it. So I decided that installing Mythbuntu 9.04 onto the new atom frontend was the way to go.\nMythbuntu 9.04 installed without any problems, although video and TV playback was very choppy, even at low resolutions. This version of mythbuntu (as it’s based on the ubuntu 9.04) did not ship with proprietary Nvidia drivers. However, downloading them from the Nvidia website and running through the installer was a fairly painless experience.\nWith the Nvidia drivers installed and running, the differences were very noticeable. Video playback was silky smooth, the board ran cooler and the power consumption dropped, now pulling just 16 watts while playing video. Prior to installing the Nvidia drivers the board pulled 21 watts.\nTo get VGA output working with my CRT TV, I first tried using a VGA to Svideo lead. For this to work you need to modify the X11 modlines. However, after an evenings faffing about I could only manage to get a black and white picture, and not a very good one at that! So in the end I bought a VGA to TV adapter. Although this ‘just works’, the picture quality is still not that great. A little more effort in setting up X11 is needed…\nFinally, I need to get a remote control working. The motherboard does come with a remote and sensor that plugs into USB. Unfortunately the remote is a little light in the button department and in Linux most of those don’t work! The motherboard has a COM connector on it, and I plan to use this to make the Hauppuage remote and sensor work.\nThese normally work with the tuner card, but as this is running in my MythTV backend server I’ll need to build a small circuit to get them working on the MythTV frontend box. Well, that’s the plan anyway!!\n","tags":["mini-atx","mythtv"],"title":"Mini ITX MythTV Frontend","uri":"/mini-itx-mythtv-frontend/"},{"categories":["Xen"],"content":"Sometimes you might need to mount a DOMU’s disk in DOM0, so that you can edit files, or even recover them in the event that your DOMU is broken and fails to boot.\nkpartx is a handy command line tool that creates device maps from partition tables. So it can be used to create device mappings (in /dev/mapper/) for file and LVM backed DOM0 disks. In this short guide, I’ll be using LVM backed disks for examples…\nThe following assumes that the logical volume is called naslv and is in volume group datavg. Amend these to fit your system.\nlist partitions in an LVM logic volume:\nkpartx -l /dev/mapper/datavg-naslv\nExample output:\ndatavg-naslv1 : 0 208782 /dev/mapper/datavg-naslv 63\u003cbr /\u003e datavg-naslv2 : 0 2088450 /dev/mapper/datavg-naslv 208845\u003cbr /\u003e datavg-naslv3 : 0 6088635 /dev/mapper/datavg-naslv 2297295\nAs you can see, each partition is number is added to the end of the device name. A this stage we’ve just listed them, the device mapper has not been amended yet.\nNow we add the partitions to the device mapper list, so that they can be mounted:\nkpartx -a /dev/mapper/datavg-naslv\nCheck that they have been added to device mapper:\nls -l /dev/mapper/datavg-naslv*\nexample output:\nbrw-rw---- 1 root disk 253, 9 2010-07-10 10:06 /dev/mapper/datavg-naslv\u003cbr /\u003e brw-rw---- 1 root disk 253, 11 2010-07-10 21:10 /dev/mapper/datavg-naslv1\u003cbr /\u003e brw-rw---- 1 root disk 253, 12 2010-07-10 21:10 /dev/mapper/datavg-naslv2\u003cbr /\u003e brw-rw---- 1 root disk 253, 13 2010-07-10 21:10 /dev/mapper/datavg-naslv3\nNow mount the required partition (filesytem) so that you can read/edit the files it contains:\nmount /dev/mapper/datavg-naslv3 /mnt/xvda3\nWhen done, unmount the partition (filesystem):\numount /mnt/xvda3\nNow to tidy up and remove the partitions we added to the device mapper list:\nkpartx -d /dev/mapper/datavg-naslv\nConfirm everything is back to pre faffing state:\nls -l /dev/mapper/datavg-naslv*\nexample output:\nbrw-rw---- 1 root disk 253, 9 2010-07-10 10:06 /dev/mapper/datavg-naslv\n","tags":["dom0","domu","LVM","Xen virtualization"],"title":"Mounting LVM backed XEN disk partitions in DOM0","uri":"/mounting-lvm-backed-xen-disk-partitions-in-dom0/"},{"categories":["Retro","Commentry"],"content":"My original website (The Sinclair Archives) was removed from the Internet in 2000, and I thought I’d lost the content forever. But while sorting through a load of old cd-roms destined for the bin I stumbled across a 10 year old backup disk that contained a copy of my old website. On the old site was a short interview I’d done with Matthew Smith, the programmer of Manic Miner and Jet Set Willy. So, here is the interview, complete with the back story as presented on my original site back in 1999\nMatthew Smith Interview – August 1999 This interview came about after I had read on the comp.sys.sinclair newsgroup that Matt was back. I decided that I should drop him an email to tell him that I’d written a remake of Styx, and would remove it from my website if he wanted me to (I’m pleased to say he didn’t!).\nHe also agreed to answer a few questions, here they are…\nBefore putting the interview onto The Sinclair Archive, I thought it would be a good idea to get some proof that it was the real Matthew Smith I had been talking to. After all, I didn’t want to misrepresent Matt by putting a false interview on the net.\nI emailed Chris Cannon, a former Software Projects programmer, who replied “It is him . . . I actually spoke to him on the phone last week, and can confirm it.”\nAnd Stuart Fotheringham, also a former Software Projects programmer, who replied “Yes, it’s definately him. He emailed me with lots of things that only the real Matthew Smith could know.”\nFirst, a question I’ve been wanting to ask you since 1984. What happened to ‘Willy meets the taxman?\nWilly meets the Taxman.\nTrilogy, trilogy gotta write a trilogy.\nManic Miner took 8 weeks from notebook to final mastering. I came back from the holiday I took with the money from Styx and went for the burn.\nJet Set Willy took 8 months, although there is little extra code. most of the time went into designing and tuning the screens. I had pressure from my partners and the bugs are due to this.\nWilly Meets the Taxman was to be the third in the series but I was spending more time sorting the problems other programmers were having than writing it. I was attempting to network all the software projects programmers together but my partners were just as busy trying to isolate us in individual sweatboxes.\nWe all know that you wrote Styx, Manic Miner and Jet Set Willy, and you also did some work on The Birds and The Bee’s. Did you work on any other games?\nThe Birds and the Bee’s was coded by Derrick Rowson, also of Wallasey, with graphics by me. He coded JSW II which had graphics by me and Steve Wetherill (now boss of Westwood Studios)\nIn the 80’s some magazines referred to you as the millionaire programmer, while there were also speculation that you were ripped off. What really happened?\nI never received a penny for JSW. When the money I got from Bug Byte for MM ran out, I couldn’t afford to keep my machines working. I was forced to use inferior compilers and one inadequate disk drive when I moved onto an ST. I was getting £50 a week (and I had to break into the office for that most weeks) and they were continually sabotaging my phone line and reputation.\nAnd finally, do you still sport the long hair and sandals?\nI chopped the hair shortly before leaving SP. I do factory work when available (i.e. when I’m not on Merseyside) so I wear boots and a no.4 crop. New pics will be on my site soon. I’m worried about how little my appearance has changed ;)\n-Matt\n","tags":["jet-set-willy","manic-miner","spectrum","styx"],"title":"Matthew Smith Interview","uri":"/matthew-smith-interview/"},{"categories":["Linux"],"content":"To convert most video files into DVD compatible mpg files, mencoder is a good option.\nThe following will convert to PAL widescreen format.\nmencoder -oac lavc -ovc lavc -of mpeg -mpegopts format=dvd -vf\u003cbr /\u003e scale=720:576,harddup -srate 48000 -af lavcresample=48000 -lavcopts vcodec=mpeg2video:vrc_buf_size=1835:vrc_maxrate=9800:vbitrate=5000:\u003cbr /\u003e keyint=15:aspect=16/9:acodec=ac3:abitrate=192 -ofps 25 -o \"OUTPUT.mpg\" \"INPUT.avi\"\nYou might want to consider using ffmpeg as an alternative. On some files I’ve had glitching as a result of converting with mencoder.\n","title":"Convert video files to DVD format using mencoder","uri":"/convert-video-files-to-dvd-format-using-mencoder/"},{"categories":["Android"],"content":"I decided it was time to change my mobile phone. Being a Linux nerd I decided to opt for an Android based phone.\nThere are quite a number to choose from. The HTC Hero being the most popular, but at the time this would have cost me £360 of my hard earned cash.\nT-Mobile have released an ‘own brand’ phone, the pulse, manufactured by Huawei. At the time of writing you can get one for under £150.\nThe pulses spec is very similar to that of the hero, but with slightly less memory, leaving just 60mb left to install new apps into. For reasons best known to google, android apps cannot be installed onto the SD card which is a real shame not least for the apps market in general.\nPlease note that rooting a phone will not, by itself, allow you to install apps onto the SD card. For that you also need to modify the ROM, or replace it with a custom ROM.\nBIG FAT WARNING Rooting your phone will almost certainly invalidate your warranty. It is also possible that you could brick your phone. The instructions below are simply a blog of what I did to my phone. There are no guarantees that they will work on your phone. So if you follow these instructions and bad things happen to your phone, don’t blame me! Remember, the risk of making your phone completely useless is yours not mine. END OF BIG FAT WARNING\nGetting root access on the phone is fairly simple.\nFirst download superboot and Amon RA from the modaco website.\nUnzip the two files into separate folders and chmod the files install-superboot-linux.sh and install-recovery-linux.sh to make them executable (e.g. chmod u+x filename).\nSwitch the phone off (“power off”)\nHold Volume Down and the red button and switch the phone on.\nThis should bring you to the USB FastBoot screen. Now attach the USB cable to the phone and PC.\nAs root run install-superboot-linux.sh (e.g. in ubuntu cd to the folder containing the superboot files and type sudo ./install-superboot-linux.sh)\nThis is enough to give you root access. However, if you also want the ability to make backups of your phone and/or install custom roms then you can also install the recovery image:\nAs root run install-recovery-linux.sh (e.g. in ubuntu cd to the folder containing the superboot files and type sudo ./install-recovery-linux.sh)\nOnce you get the successful message you can restart the phone. I waited a couple of mins, “just in case”. The only way I could find to get out of the fastboot screen was to remove the battery.\nAnd that’s about it.\nTo test that you have root access you need to install a ssh client, such as ConnectBot and shell onto localhost. When you do an su to gain root access, a pop-up window should ask you if you want to allow root access or not.\nIf you’ve also installed the recover image then you use the quickboot icon that should now be in your apps list. This will re-boot your phone into the recovery menu where you can backup and restore images as well as a few other options.\n","tags":["Android","phone","rooting","t-mobile"],"title":"Rooting the T Mobile Pulse phone","uri":"/rooting-the-t-mobile-pulse-phone/"},{"categories":["Linux"],"content":"I’ve just installed the 64bit version of Ubuntu 9.04 onto my main desktop PC. Everything was going well until I wanted to print a document.\nNo, my printer was correctly detected and when I clicked ‘print’ the printer burst into life as expected. But after printing about half of the first page the printer just stopped. My printer is a HP Deskjet F2100 series (F2180 to be precise).\nI tried everything, google didn’t throw up anything useful, I installed later HP drivers but noting fixed it.\nAfter a lot of head scratching ad experimenting I did notice something odd. The USB device as owned by a user I did not have permissions for. I wasn’t expecting this to be the cause of the problem because if it was due to permissions then surely the printer would not print at all?!?\nI was wrong, and adding myself to the group that owned the USB device has fixed it. As this seemed quite off the wall I thought I’d share…\nSo, first, see where the printer is connected:\n[sourcecode language=“bash”]lsusb\nBus 001 Device 003: ID 0dda:2027 Integrated Circuit Solution, Inc. USB 2.0 Card Reader\nBus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub\nBus 002 Device 003: ID 03f0:7d04 Hewlett-Packard Deskjet F2100 Printer series\nBus 002 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub[/sourcecode]\nSo its bus 002, device 003.\nNext lets have a look at that device:\n[sourcecode language=“bash”]ls -l /dev/bus/usb/002/003\ncrw-rw—-+ 1 lp lp 189, 130 2009-08-30 17:25 /dev/bus/usb/002/003[/sourcecode]\nSo the group is lp\nNext add the group lp to any users that need access to the printer. Remember to user the -a switch to append to the existing group list.\n[sourcecode language=“bash”]usermod -aG lp userid[/sourcecode]\nYou’ll then need to log off and back on for the permissions to take effect.\n","title":"HP Deskjet F2180 stops after printing half of a page","uri":"/hp-deskjet-f2180-stops-after-half-page-print/"},{"categories":["Xen"],"content":"For distros that don’t support XEN compatible installers the easiest way to create a PV (ParaVirtual) DOMU is to install as a HVM (see my other article entry for this) ad then once installed, convert it to a PV DOMU.\nThere are several reasons why you might want to go with PV instead of HVM. The main two that spring to mind are PV’s offer better performance/lower overhead and PV DOMU’s allow PCI passthrough. Although apparently XEN 3.4 can now do PCI passthrough with HVM, as long as you have an Intel chip with vt-d.\nThis article will start off where my previous “Installing a XEN hvm DOMU” left off…\nThe description below uses Ubuntu 9.04. If your using a different distro your mileage may vary.\nMost modern kernels will already support XEN PV, but without PCI passthrough. If you don’t need to pass any PCI devices directly to the DOMU then you can skip this next bit…\nTo install a full xen compatible kernel, taken from Debian:\nWith the DOMU booted as a HVM, add the following lines to the end of /etc/apt/sources.list\n# Added for Debian xen kernel deb http://ftp.debian.org/debian/ lenny main deb-src http://ftp.debian.org/debian/ lenny main deb http://ftp.debian.org/debian/ lenny-proposed-updates main Now start synaptic from the system/administration menu, and click on the reload button to refresh the package lists. Ignore the warning about the new Debian repositories not being verified.\nNow hit search and enter linux-image-2.6.26-2-xen and click to install it (it will also install the modules automatically).\nNow we need to edit the boot menu, to add a disk option so that pvgrub can find the disk. If you’ve installed the XEN specific kernel as above then we’ll also add an entry for that too.\nEdit /boot/grub/menu.lst\nTo convert the standard kernel copy the code block for the first boot option and then add the root (hd0,0) line (dont just copy the whole lot below, as your uuids will differ)\ntitle Ubuntu 9.04, kernel 2.6.28-11-generic\u003cstrong\u003e-PV\u003c/strong\u003e uuid 265b80ca-0896-48a7-913a-6f5df64776bc \u003cstrong\u003eroot (hd0,0)\u003c/strong\u003e kernel /boot/vmlinuz-2.6.28-11-generic root=UUID=265b80ca-0896-48a7-913a-6f5df64776bc ro quiet splash initrd /boot/initrd.img-2.6.28-11-generic quiet And for the Debian XEN kernel:\ntitle Ubuntu 9.04, kernel 2.6.26-2\u003cstrong\u003e-xen\u003c/strong\u003e uuid 265b80ca-0896-48a7-913a-6f5df64776bc \u003cstrong\u003eroot (hd0,0)\u003c/strong\u003e kernel /boot/\u003cstrong\u003evmlinuz-2.6.26-2-xen-amd64\u003c/strong\u003e root=UUID=265b80ca-0896-48a7-913a-6f5df64776bc ro quiet splash initrd /boot/\u003cstrong\u003einitrd.img-2.6.26-2-xen-amd64\u003c/strong\u003e quiet Note that the above xen kernel and initrd are for the 64bit version, use 2.6.26-2-686 for 32bit.\nWhile you at it you might want to install openssh-server so that you can shell onto the DOMU later.\nYou may also consider going with a fixed IP address so that you can find your DOMU when you want to shell onto it!\nedit /etc/network/interfaces\n# The primary network interface auto eth0 iface eth0 inet static address 192.168.1.xxx netmask 255.255.255.0 gateway 192.168.1.1 Now we’re ready to change the config file on the DOM0, so shut down the DOMU.\nEdit you XEN DOMU config file and replace:\nkernel = \"/usr/lib/xen/boot/hvmloader\" builder='hvm' with:\nkernel = \"/usr/lib/xen/boot/pv-grub-x86_64.gz\" extra = \"(hd0,0)/boot/grub/menu.lst\" Restart your DOMU and it should boot into a PV kernel. Now you can do neat things like PCI passthrough! (I’ll knock up another blog for that…)\n","tags":["hvm","paravirtual","pv","Xen"],"title":"Converting a XEN DOMU from HVM to PV","uri":"/converting-a-xen-domu-from-hvm-to-pv/"},{"categories":["Linux","Xen"],"content":"For this to work your CPU and motherboard needs to support vt (intel) or amd-v. You can check this by looking at the flags in /proc/cpuinfo. Your looking for either vmx or svm on the flags line.\nIf you’re doing this from your DOM0 then these flags may not actually show up. Instead type xm info and look for ‘hvm’ on the virt_caps line.\nFor my hvm install, i’ll be installing Ubuntu 9.04 Desktop Linux.\nI have copied the live CD ISO onto my XEN DOM0 and will be installing directly from the iso rather than burning the CD. I have also created a 10GB LVM logical volume to act as the disk for my new DOMU.\nThe following config file is used to boot the live cd (iso image). It lives in /etc/xen and as this is to become a myth frontend i’ve rather imaginatively called it mythfronted!\n/etc/xen/mythfrontend:\nkernel = \"/usr/lib/xen/boot/hvmloader\" builder='hvm' memory = 512 name = \"mythfrontend\" vif = [ 'type=ioemu, bridge=eth0' ] disk = ['file:/xenimages/ISO/ubuntu-9.04-desktop-i386.iso,hda:cdrom,r', 'phy:/dev/mapper/datavg-mythfrontendlv,sda,w', ] boot=\"d\" sdl=0 vnc=1 vnclisten=\"0.0.0.0\" vncpasswd='password' stdvga=0 serial='pty' usbdevice='tablet' I find that setting usbdevice to tablet rather than mouse gives better results with a mouse than setting it to mouse.\nOnce you have your config file created and matching your system (i.e. pointing to the cd or iso, and file of lvm for hard disk) then we’re good to boot it:\nxm create mythfrontend\nIf all went well then we should see it running in xm list\nNow we need to connect to it’s display with vnc.\nIf this is the first DOMU to use vmc as a framebuffer then you should be able to run vnc to get access to the display. In the config above I have defined a password to be entered.\nIf you need to specify the port, then typing netstat -tap should let you see which port (probably 5900 ish).\nOnce you have connected to via VNC you will then be able to run the installer as usual. This should work for installing Windows as well as Linux.\nAfter the install has completed, shutdown the new DOMU, because we need to edit the config file to remove the cd iso image and allow it to boot from it’s hard disk.\nThis basically means removing the cd-rom/iso section in the disk= parameter, change the device name for the hard disk from sda to xvda and removing boot=”d” completely. This should leave our config looking something like this:\nkernel = \"/usr/lib/xen/boot/hvmloader\" builder='hvm' memory = 512 name = \"mythfrontend\" vif = [ 'type=ioemu, bridge=eth0' ] disk = [ 'phy:/dev/mapper/datavg-mythfrontendlv,xvda,w', ] sdl=0 vnc=1 vnclisten=\"0.0.0.0\" vncpasswd='password' stdvga=0 serial='pty' usbdevice='tablet' Now just start the DOMU with xm create mythfrontend\nAnd that’s it!\n","tags":["domu","hvm","Xen"],"title":"Installing a XEN hvm DOMU","uri":"/installing-a-xen-hvm-domu/"},{"categories":["Linux"],"content":"This is a quick guide to setting up a Linux software RAID mirror (aka RAID 1).\nThere are a couple of toolsets for managing raid on Linux, raidtools and mdadm. This guide will use mdadm because imho it has better commands and features for monitoring.\nRight, first you’ll need a couple of disk partitions of about the same size. These should be on separate disks or you’ll be defeating the point of raid mirroring Raid disks appear under /dev as md0, md1 etc.\nA quick way to check the status of your raid devices is to type cat /proc/mdstat\nOn my system this shows:\nmd1 : active raid1 sda2[0] sdb2[1]\n96320 blocks [2/2] [UU]\nmd0 : active raid1 sda1[0] sdb1[1]\n29294400 blocks [2/2] [UU]\nBecause on my system md0 and md1 are already in use, I will create this new raid as md2.\nSo lets assume that you have 2 partitions; /dev/sda3 and /dev/sdb3. The following command will create a raid device as /dev/md2.\nmdadm --create --verbose /dev/md2 --level=mirror --raid-devices=2 /dev/sda3 /dev/sdb3\nIf all went well you can confirm the size/details of your new raid device:\nmdadm --detail /dev/md2\n/dev/md2:\nVersion : 00.90\nCreation Time : Wed Apr 22 21:34:44 2009\nRaid Level : raid1\nArray Size : 458993024 (437.73 GiB 470.01 GB)\nUsed Dev Size : 458993024 (437.73 GiB 470.01 GB)\nRaid Devices : 2\nTotal Devices : 2\nPreferred Minor : 2\nPersistence : Superblock is persistent\nUpdate Time : Wed Apr 22 21:34:44 2009\nState : clean\nActive Devices : 2\nWorking Devices : 2\nFailed Devices : 0\nSpare Devices : 0\nUUID : e893056a:5de3d066:4bca1532:59cc93a2 (local to host xenmaster)\nEvents : 0.1\nNumber Major Minor RaidDevice State\n0 8 3 0 active sync /dev/sda3\n1 8 19 1 active sync /dev/sdb3\nAnd you cat check that it is running:\ncat /proc/mdstat\nmd2 : active (auto-read-only) raid1 sdb3[1] sda3[0]\n458993024 blocks [2/2] [UU]\nresync=PENDING\nmd1 : active raid1 sda2[0] sdb2[1]\n96320 blocks [2/2] [UU]\nmd0 : active raid1 sda1[0] sdb1[1]\n29294400 blocks [2/2] [UU]\nIn my case you can see that the raid pair is not synced yet. After about 10 minutes or so re-issuing the command shows that the raid pair are now syncing (alternatively you can try running the raid to kick start the sync by issuing mdadm --run /dev/md2):\ncat /proc/mdstat\nmd2 : active raid1 sdb3[1] sda3[0]\n458993024 blocks [2/2] [UU]\n[=\u003e……………….] resync = 7.6% (35247744/458993024) finish=73.2min speed=96456K/sec\nand mdadm -detail /dev/md2\n/dev/md2:\nVersion : 00.90\nCreation Time : Wed Apr 22 21:34:44 2009\nRaid Level : raid1\nArray Size : 458993024 (437.73 GiB 470.01 GB)\nUsed Dev Size : 458993024 (437.73 GiB 470.01 GB)\nRaid Devices : 2\nTotal Devices : 2\nPreferred Minor : 2\nPersistence : Superblock is persistent\nUpdate Time : Wed Apr 22 22:10:29 2009\nState : active, resyncing\nActive Devices : 2\nWorking Devices : 2\nFailed Devices : 0\nSpare Devices : 0\nRebuild Status : 9% complete\nUUID : e893056a:5de3d066:4bca1532:59cc93a2 (local to host xenmaster)\nEvents : 0.3\nNumber Major Minor RaidDevice State\n0 8 3 0 active sync /dev/sda3\n1 8 19 1 active sync /dev/sdb3\nAt this point is should be possible to mount and use the raid device and even format it while it is being rebuilt. However, I want to put LVM on it so I’ll wait until the sync is complete…\nPlease note that the new raid device will most likely sow in /proc/mdstat as active (auto-read-only) until you either mount it or create an LVM volume on it (i.e. use it).\nAnd that’s it really. You use /dev/md2 as you would any disk partition such as /dev/sda1 to format, mount or use as a PV for LVM.\n","tags":["raid","raid-1","raid"],"title":"How to set up a Linux RAID mirror","uri":"/how-to-set-up-a-linux-raid-mirror/"},{"categories":["Linux"],"content":"Sometimes I find it useful to switch to hex mode when editing a file in vi. The command for switching is not very obvious so thought I’d share…\nSo, open a file in vi as usual. To switch into hex mode hit escape and type: :%!xxd\nAnd when your done and want to exit from hex mode hit escape again and type: :%!xxd -r\nOkay, so this isn’t actaully switching to vi’s ‘hex mode’; vi doesn’t have one. What the above actually does is to stream vi’s buffer through the external program ‘xxd’.\n","tags":["hex","vi"],"title":"Using vi as a hex editor","uri":"/using-vi-as-a-hex-editor/"},{"categories":["Linux","Xen"],"content":"If you’ve followed my guide to installing xen from source then you might now be wondering how to create your first VM.\nThis guide will show you how to manually create and install a VM (known as a DOMU instance in Xen) without any additional tools such as virt-manager, virt-install or virsh. These tools can help the novice, but are quite limiting when it comes to the advance features of xen.\nFor this example I’ll be installing CentOS5, but this should work for any red-hat based distro (please note there is a bug in fedora10 that stops it booting under xen). This example also uses the paravirtual kernel so it will work even if your CPU does not support AMD-V or intel-VT.\nRight, down to business. The first thing we need to do is create a file to user as the hard disk for the DOMU. There are other options for providing hard disks to a DOMU, and I’ll cover some of them in a later guide but for this install I’ll be using a file.\nThe command below will create an 8GB file that will be used as an 8GB drive. The whole file will be written to disk in one go so may take a short while to complete.\ndd if=/dev/zero of=/xenimages/test01/disk1.img oflag=direct bs=1M count=8192\nAlternatively, you can use the command below to create the same size file as a sparse file. What this does is create the file, but only take up disk space as the file is used. In this case the file will only really take about 1mb of disk initially and grow as you use it.\ndd if=/dev/zero of=/xenimages/test01/disk1.img oflag=direct bs=1M seek=8191 count=1\nThere are pros and cons of using sparse files. On one hand they only take as much disk as is actually used, on the other hand the file can become fragmented and you could run out of real disk if you overcommit space.\nNext up we’ll mount the install CD and export it over nfs so that xen can use it as a network install.\nmkdir /tmp/centos52\u003cbr /\u003e mount /dev/hda /tmp/centos52 -o loop,ro\nJust to check the mount went OK: ls /tmp/centos52 should show the files.\nNow run the export:\nexportfs *:/tmp/centos5\nNow we’ll create the xen config file for our new instance. The default location for xen config files is /var/xen so that’s where ours will go.\nI’m going to call my VM test01, so I’ll create a file /var/xen/test01 that contains the following initial configuration:\nkernel = \"/tmp/centos52/images/xen/vmlinuz\" ramdisk = \"/tmp/centos52/images/xen/initrd.img\" name = \"test01\" memory = \"256\" ## disk = [ 'tap:aio:/xenimages/test01/disk1.img,xvda,w', ] disk = [ 'file:/xenimages/test01/disk1.img,xvda,w', ] vif = [ 'bridge=eth0', ] vcpus=1 on_reboot = \"destroy\" on_crash = \"destroy\" Note that if you are installing from a different machine from your xen machine the you will need to nfs mount the install disk in order for the above config to kick off the installer. e.g.\nmount IP address:/tmp/centos52 /tmp/centos52\nSo, lets boot the new instance and start the installer.\nxm create test01\nAfter a moment or two the console should return with something like “Started domain test01″.\nNow lets connect to the console and proceed with the install:\nxm console test01\nOr if you prefer the previous two commands can be combined into one: xm create test01 -c.\nFrom here on you should work through the standard text mode installer.\nThe points to note are:\n** For installation image select “NFS image”.** Then in the later nfs panel enter your PC’s IP address for the servername and /tmp/centos52 (or wherever you mounted the cd) as the directory. I also specified a manual IP address for my VM. I selected my routers IP for the gateway and dns server, so that I can access the internet from the VM later. The hard drive is named xvda, as specified in the config file. This will need to be partitioned and formatted by the installer. The rest of the install is fairly straight forward. If in doubt just go with the defaults, although it’s probably a good idea to set a manual IP address in your subnet range so that you can easily ssh onto the VM.\nNote that to release the console from your VM hold down Ctlr and press the ] key.\nWhen the install is complete the new domain will need to be shut down (you’ll be prompted to ‘restart’ by the installer, this will in fact shut down the VM because we set the on_reboot option to destroy), and then the xen config file must be modified to allow the new VM to boot.\nSo, edit the config file that we created earlier and comment out the kernel and ramdisk lines. You should also change the on_crash and on_reboot actions to restart.\nSo the edited config file now looks like this:\n\u003cstrong\u003e## kernel = \"/tmp/centos52/images/xen/vmlinuz\" ## ramdisk = \"/tmp/centos52/images/xen/initrd.img\"\u003c/strong\u003e name = \"test01\" memory = \"256\" ## disk = [ 'tap:aio:/xenimages/test01/disk1.img,xvda,w', ] disk = [ 'file:/xenimages/test01/disk1.img,xvda,w', ] vif = [ 'bridge=eth0', ] vcpus=1 \u003cstrong\u003eon_reboot = \"restart\" on_crash = \"restart\"\u003c/strong\u003e Finally we can boot the new VM instance:\nxm create test01 -c\nand log in as root. You should also be able to ssh onto it from your network.\nAnd that’s about it!\nOne last note. If you want the VM to start and stop with your xen server, just move the config file into the auto sub folder (or create a symbolic link to it).\n","tags":["domu","Xen"],"title":"Manually creating a Xen DOMU instance","uri":"/manually-creating-a-xen-domu-instance/"},{"categories":["Xen"],"content":"This is how I built my Xen server.\nFirst install Debian 5.0 (Lenny). This should also work on 4 (Etch) too.\nEverything is done on the command line as root.\nTo build Xen from source you’ll need as fair number of dependencies. The easiest way to install them is by using the apt-get install command as below:\napt-get install bcc bin86 gawk bridge-utils iproute libcurl3 libcurl4-openssl-dev bzip2 module-init-tools transfig tgif texinfo pciutils-dev mercurial build-essential make gcc libc6-dev zlib1g-dev python python-dev python-twisted libncurses5-dev patch libvncserver-dev libsdl-dev libjpeg62-dev\nIf you installed the 64bit version of Debian then you’ll also need gcc-multilib for the compile to work:\napt-get install gcc-multilib\nNext create a temporary folder and download the Xen source code:\nmkdir xentemp\u003cbr /\u003e cd xentemp\u003cbr /\u003e wget http://bits.xensource.com/oss-xen/release/3.3.1/xen-3.3.1.tar.gz\nNow extract the soucecode from the downloaded archive:\ntar -xzf xen-3.3.1.tar.gz\u003cbr /\u003e cd xen-3.3.1\nRight, now on with the compile (this will take a while…):\nmake world\nNote, if you want USB and PCI passthrough to answer Y when prompted.\nAssuming that the above compile went ok:\nmake dist\nAnd now install the newly compiled kernel and tools into the system:\n./install.sh\nEnsure the xen deamon starts and stops with the rest of the system:\nupdate-rc.d xend defaults 20 21\u003cbr /\u003e update-rc.d xendomains defaults 21 20\nCreate the module dependencies stuff:\ndepmod 2.6.18.8-xen\nUpdate/create initramfs for the new kernel:\nupdate-initramfs -c -k 2.6.18.8-xen Add the new kernel to your grub boot menu:\nupdate-grub\nReboot the system and hope that it works!\nreboot\nNow have a xen kernel:\nuname-a\nShould show a xen kernel at version 2.6.18\nHowever on my gigabyte motherboard I was getting some boot time errors:\n2.6.18 exception Emask 0×40 SAct 0×0 SErr 0×800 action 0×2\nA quick google confirmed that this was due to the old kernel version, 2.6.18 and my motherboard.\nSo to fix the error I used synaptic to install latest xen kernel from lenny repositories…\nuname -a\n2.6.26-1-xen-amd64 #1 SMP Fri Mar 13 21:39:38 UTC 2009 x86_64 GNU/Linux\nThis resolved the error and still leaves me with xen 3.3.1\nType xm info to confirm xen version\nAnd that’s it!\n","tags":["debian","xen","virtualization"],"title":"Installing Xen 3.3.1 from source on Debian 5.0 (Lenny)","uri":"/installing-xen-33-from-source-on-debian-50-lenny/"},{"categories":["Linux"],"content":"This is a quick and dirty guide to setting up an LVM based filesystem.\nFirst create some disk partitions. In this example I’ve got two identical unformatted partitions on a couple of hard drives. These appear in /dev as sda4 and sdb4. Please note your partition number will most likely be different.\nAlso note that it’s easy to break your system with some of these commands. You have been warned, so don’t blame me if you wipe your system out!\nFirst make the partitions available to LVM:\npvcreate /dev/sda4\nPhysical volume “/dev/sda4″ successfully created\npvcreate /dev/sdb4\nPhysical volume “/dev/sdb4″ successfully created\nNext create a volume group:\nvgcreate xenimagesvg /dev/sda4 /dev/sdb4\nVolume group “xenimagesvg” successfully created\nWe can check the size etc of the new volume group with vgdisplay:\nvgdisplay xenimagesvg\n— Volume group —\nVG Name xenimagesvg\nSystem ID\nFormat lvm2\nMetadata Areas 2\nMetadata Sequence No 1\nVG Access read/write\nVG Status resizable\nMAX LV 0\nCur LV 0\nOpen LV 0\nMax PV 0\nCur PV 2\nAct PV 2\nVG Size 289.52 GB\nPE Size 4.00 MB\nTotal PE 74116\nAlloc PE / Size 0 / 0\nFree PE / Size 74116 / 289.52 GB\nVG UUID m408F0-77d6-WXwU-ykD9-5OCB-pIkG-cSbIDP\nNext, we need to create a logical volume in our new volume group:\nlvcreate -n xenimageslv -L 250G xenimagesvg\nThe above will create a logical volume called xenimageslv in volume group xenimagesvg\nNow we need to format the logical volume so that it can be used as a filesystem.\nI’m going to format to ext3 using mke2fs with -j (for journalling, aka ext3):\nmke2fs -j /dev/mapper/xenimagesvg-xenimageslv\nmke2fs 1.41.3 (12-Oct-2008)\nFilesystem label=\nOS type: Linux\nBlock size=4096 (log=2)\nFragment size=4096 (log=2)\n16384000 inodes, 65536000 blocks\n3276800 blocks (5.00%) reserved for the super user\nFirst data block=0\nMaximum filesystem blocks=4294967296\n2000 block groups\n32768 blocks per group, 32768 fragments per group\n8192 inodes per group\nSuperblock backups stored on blocks:\n32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208,\n4096000, 7962624, 11239424, 20480000, 23887872\nWriting inode tables: done\nCreating journal (32768 blocks): done\nWriting superblocks and filesystem accounting information: done\nThis filesystem will be automatically checked every 32 mounts or\n180 days, whichever comes first. Use tune2fs -c or -i to override.\nAnd finally we need to mount our new filesystem so that we can actually use it. Here I’ll add the entry into fstab so that it mounts on system startup.\nCreate a folder to mount the filesystem into:\nmkdir xenimages\nedit fstab and add the following line to mount our new lvm filesystem at system startup…\n/dev/mapper/xenimagesvg-xenimageslv /xenimages ext3 errors=remount-ro 0 0\nNow run mount-a to re-run fstab mounts and mount the new filesystem\nCheck it’s mounted with df -h:\ndf -h\nFilesystem Size Used Avail Use% Mounted on\n/dev/mapper/rootvg-rootlv\n21G 4.5G 15G 24% /\ntmpfs 1.9G 0 1.9G 0% /lib/init/rw\nudev 10M 124K 9.9M 2% /dev\ntmpfs 1.9G 0 1.9G 0% /dev/shm\n/dev/md1 92M 51M 36M 59% /boot\n/dev/mapper/xenimagesvg-xenimageslv\n247G 188M 234G 1% /xenimages\n","tags":["LVM","LVM2"],"title":"LVM how to","uri":"/lvm-how-to/"},{"categories":["Linux"],"content":"As you might expect, the software that accompanied my Everio HDD camcorder when I bought it was not compatible with Linux. Just as well we don’t actually need any special software to download recordings onto your favourite OS (yes I do mean Linux!)\nPlug everything in, USB and the power adapter then switch the camera into PLAY mode.\nAt this point you should be presented with the usual KDE or GNOME window and be able to view/copy the files in the same way as you would a USB memory stick.\nHowever, there are a couple of drawbacks with this. First, just dragging the files will create a copy on your PC with timestamps of when you copied the videos, not actually filmed them. Second, the video files on your camcorder end in .mod and not .mpg which would be preferable for compatible playback.\nSo we’ll be using the command line to copy recording and rename them. There’s not much to it really…\nOpen a terminal and cd into the directory that contains the mount point for the camcorder. In my case this was /media/EVERIO_HDD.\nThe video files are nested a little deeper in SD_VIDEO/PRG001.\n(e.g. cd /media/EVERIO_HDD/SD_VIDEO/PRG001)\nThis folder contains two types of files ending in .mod and .moi. We only need the .mod files as these are the actual videos.\nTo copy them from the camcorder to your hard drive use something along the line of:\nmkdir /home/my_vids/downloaded_2009-03-01\u003cbr /\u003e cp -p *.MOD /home/my_vids/downloaded_2009-03-01/\nThis can take a while as the files can be quite large. Note that the -p option tells the cp command to preserve the file timetamps, which will have been set by the camcorder when the recording was made.\nI prefer the files to have a suffix of .mpg instead of .MOD. Run the following rename command from within your downloaded folder to convert them.\ncd \u003ccode\u003e/home/my_vids/downloaded_2009-03-01\u003cbr /\u003e rename 's/.MOD$/.mpg/' *.MOD\nAnd thats about it. You will still need to delete them from the camcorder (best done from the camcorder menu). Also remember to keep backups…\n","tags":["Camcorder","Linux"],"title":"Copying JVC Everio HDD camcorder recordings with Linux","uri":"/copying-jvc-everio-hdd-camcorder-recordings-with-linux/"},{"categories":["Linux"],"content":"There are many GUI programs that allow you to convert avi/divx/xvid files into DVD format, such as DeVeDe, but I somehow prefer to use command line tools. I know, I’m just weird like that!\nThere are 3 basic steps in creating a simple DVD:\nConvert the video files create the DVD structure Burn the DVD So, to convert a file to a DVD compatible format use something like:\nffmpeg -i \"non dvd format input video.avi\" -y -target pal-dvd -sameq -aspect 16:9 output.mpg\nNext we need to create the DVD structure. We’ll use dvdauthor for this.\nRather than pass eveything into dvdauthor via the command line, it’s easier to put the settings into an xml file.\nSo, create a file (e.g. dvd.xml) and paste the lines below into it. This is about as simple a DVD structure as you can get, giving you a simple auto play disc that contains only one movie and no chapters.\n\u003cdvdauthor\u003e\u003cbr /\u003e \u003cvmgm /\u003e\u003cbr /\u003e \u003ctitleset\u003e\u003cbr /\u003e \u003ctitles\u003e\u003cbr /\u003e \u003cpgc\u003e\u003cbr /\u003e \u003cvob file=\"output.mpg\"/\u003e\u003cbr /\u003e \u003c/pgc\u003e\u003cbr /\u003e \u003c/titles\u003e\u003cbr /\u003e \u003c/titleset\u003e\u003cbr /\u003e \u003c/dvdauthor\u003e\u003cbr /\u003e then create a folder to put the structure in:\nmkdir dvd\nand then run dvdauthor, referencing the xml file created above:\ndvdauthor -o dvd -x dvd.xml\nNote that on newer versions of dvdauthor, you may need to create an export variable for the video format before you run the above command. So if you get an error run:\nexport VIDEO_FORMAT=NTSC\nor\nexport VIDEO_FORMAT=PAL\nOnce dvdauthor has done it’s magic you can test the DVD structure before burning t disc with mplayer:\nmplayer dvd:// -dvd-device ./dvd\nNow all that remains is to burn the dvd folder to a DVD disc so it will play on a standard under the telly DVD player. For this we’ll use growisofs:\ngrowisofs -dvd-compat -Z /dev/hdc -dvd-video ./dvd/\nRemember to change /dev/hdc to whatever your dvdrw device is. You need to use the device, not the mount point (hint: it’ll be /dev/something, not /media).\n","tags":["convert","dvd","ffmpeg"],"title":"Converting a video file to DVD with ffmpeg","uri":"/converting-a-video-file-to-dvd-with-ffmpeg/"},{"content":"Welcome to my humble corner of the web. This site originally began as a sort of world readable note pad of my Linux and IT related tinkering. I figured that some of it might be useful to other Linux enthusiasts, so I made the site public.\nI make no apologies for the obvious bias towards Linux and open source software. I believe freedom in software is a truly great thing.\nKev.\n","title":"About Kev's Site","uri":"/about/"},{"content":"Placeholder for sitemap generation\n","title":"bloxed","uri":"/bloxed/"},{"content":"Placeholder for sitemap generation\n","title":"bloxed","uri":"/retrocadia-web/"}]
